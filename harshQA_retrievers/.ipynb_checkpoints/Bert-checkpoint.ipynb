{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import uuid\n",
    "import prettytable\n",
    "import time\n",
    "import cProfile\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tinyarray\n",
    "import nltk\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from tika import parser\n",
    "from nltk import tokenize as tkn\n",
    "from string import digits\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as skf\n",
    "from sklearn.base import BaseEstimator\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from models import InferSent\n",
    "import enchant\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import spherecluster\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRetriever(BaseEstimator):\n",
    "    \n",
    "    def __init__(self,top_n=5):\n",
    "        self.top_n = top_n\n",
    "        self.bert=SentenceTransformer('bert-large-nli-stsb-mean-tokens')\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "        \"\"\" X: any iterable which contains words to finetune vocabulary \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        \"\"\" X: any iterable which contains sentences to embed \"\"\"\n",
    "        self.embeddings = self.bert.encode(list([s for s in X ]))\n",
    "        self.reduced_embeddings = np.apply_along_axis(lambda v: self.normalize(v), 1,self.embeddings)\n",
    "        return self\n",
    "    \n",
    "    def normalize(self,array):\n",
    "        return array/np.linalg.norm(array)\n",
    "    \n",
    "    def predict(self,X,metadata):\n",
    "        \n",
    "        question=self.bert.encode([X])\n",
    "        encoded_question=self.normalize(question)\n",
    "        self.reduced_question=self.normalize(question)\n",
    "        \n",
    "        reduced_embeddings=self.reduced_embeddings[metadata[0]:metadata[1],:]\n",
    "        \n",
    "        data=reduced_embeddings.dot(self.reduced_question.T)\n",
    "        self.scores_inf=pd.DataFrame(data,index=range(len(data)))\n",
    "        closest_docs_indices = self.scores_inf.sort_values(by=0, ascending=False).index[:self.top_n].values\n",
    "        return closest_docs_indices,self.scores_inf\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import uuid\n",
    "import prettytable\n",
    "import time\n",
    "import cProfile\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tinyarray\n",
    "import nltk\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from tika import parser\n",
    "from nltk import tokenize as tkn\n",
    "from string import digits\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as skf\n",
    "from sklearn.base import BaseEstimator\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from models import InferSent\n",
    "import enchant\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tfidf_Cluster_Bert_Finetuned(BaseEstimator):\n",
    "    \"\"\"\n",
    "    A scikit-learn estimator for TfidfRetriever. Trains a tf-idf matrix from a corpus\n",
    "    of documents then finds the most N similar documents of a given input document by\n",
    "    taking the dot product of the vectorized input document and the trained tf-idf matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ngram_range : bool, optional\n",
    "        [shape of ngram used to build vocab] (the default is False)\n",
    "    max_df : bool, optional\n",
    "        [while building vocab delete words that have a frequency>max_df] (the default is False)\n",
    "    stop_words : str, optional\n",
    "        ['english is the only value accepted'] (the default is False)\n",
    "    paragraphs : iterable\n",
    "        an iterable which yields either str, unicode or file objects\n",
    "    top_n : int\n",
    "        maximum number of top articles to retrieve\n",
    "        header should be of format: title, paragraphs.\n",
    "    verbose : bool, optional\n",
    "        If true, all of the warnings related to data processing will be printed.\n",
    "    Attributes\n",
    "    ----------\n",
    "    vectorizer : TfidfVectorizer\n",
    "        See https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "    tfidf_matrix : sparse matrix, [n_samples, n_features]\n",
    "        Tf-idf-weighted document-term matrix.\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> retriever = TfidfRetriever(ngram_range=(1, 2), max_df=0.85, stop_words='english')\n",
    "    >>> retriever.fit(X=df['content'])\n",
    "    \n",
    "    >>> doc_index=int(input('Which document do you want to use for your question?'))\n",
    "    >>> retriever.transform(X=df.loc[doc_index,'content'])\n",
    "    \n",
    "    >>> Q=str(input('Enter your question'))\n",
    "    >>> Q=retriever.vectorizer.transform([Q])\n",
    "    >>> closest_docs,scores = self.retriever.predict(newQst,df.loc[doc_index,'content'])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, TF_emb, q_emb, content, content_doc,stemmed_content, vocab, l_querries,l_querries_raw, by_querries=True, top_n=5,threshold_w=0.002, portion_w=1.0,rank=500,verbose=False):\n",
    "\n",
    "        self.top_n = top_n\n",
    "        self.verbose = verbose\n",
    "        self.TF_emb=TF_emb\n",
    "        self.threshold_importance=threshold_w\n",
    "        self.prop_w=portion_w\n",
    "        self.rk=rank\n",
    "        self.by_querries=by_querries\n",
    "        self.querries=l_querries.copy() #(dataframe querries with word importance)\n",
    "        self.querries_raw=l_querries_raw.copy()\n",
    "        self.content=content\n",
    "        self.content_doc=content_doc\n",
    "        self.stemmed_content=stemmed_content\n",
    "        self.vocab=vocab\n",
    "        self.dic_emb={}\n",
    "        self.q_emb=q_emb\n",
    "    \n",
    "    def select_term_Naystrom(self,T):\n",
    "\n",
    "        #T is the Tf-Idf Matrix \n",
    "        #s is the fraction of important words that we want to retrieve (s=1.0 generally)\n",
    "        print('begin of term selection')\n",
    "        #Retrieve all the stems of tf-idf vocabulary\n",
    "        terms=self.vocab\n",
    "        \n",
    "        #First we remove digits of the terms candidate\n",
    "        n=int(self.prop_w*len(terms))\n",
    "        idx_words=[]\n",
    "        for i,term in enumerate(terms):\n",
    "            try: float(term)\n",
    "            except: idx_words.append(i)\n",
    "\n",
    "        #Secondly we set a threshold to select words that appear at least each p documents\n",
    "\n",
    "        #Set threshold to select words that appear at least each p documents \n",
    "        self.freq_term=np.zeros((len(self.content),len(terms)))\n",
    "        \n",
    "        for j,stem in enumerate(self.stemmed_content):\n",
    "            for i,c in enumerate(terms):\n",
    "                if c in stem:\n",
    "                    self.freq_term[j,i]+=1\n",
    "                    \n",
    "        self.freq_term_by_doc=np.apply_along_axis(lambda x: np.mean(x),0,self.freq_term)\n",
    "        ids=np.where(self.freq_term_by_doc>self.threshold_importance )[0]\n",
    "        idx_words=[ i for i in ids if i in idx_words]\n",
    "\n",
    "\n",
    "        #Init selection of terms with most correlation \n",
    "        if self.prop_w!=1.0:\n",
    "            prob=np.sum(np.abs(T)>0,axis=0)\n",
    "            prob=prob/np.sum(prob)\n",
    "            prob=np.squeeze(prob)\n",
    "            idx=[]\n",
    "            m=len(idx_words)\n",
    "\n",
    "            for i in range(n):\n",
    "\n",
    "                p=int(np.random.choice(m,1,prob[0]))\n",
    "                idx.append(int(idx_words[p]))\n",
    "                prob[idx_words[p]]=0\n",
    "                prob=prob/np.sum(prob)\n",
    "\n",
    "            return idx,np.squeeze(T[:,idx])\n",
    "        \n",
    "        else:\n",
    "            idx=idx_words\n",
    "            self.idx=idx\n",
    "            return idx,np.squeeze(T[:,idx])\n",
    "        print('end of term selection')\n",
    "    \n",
    "    \n",
    "\n",
    "    def fit(self, X, y=None): \n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self): #generate new enhanced tf-idf-farahat features ( co-occurence weight frequency matrix) \n",
    "        \n",
    "        idx,A=self.select_term_Naystrom(self.TF_emb.toarray())   \n",
    "        \n",
    "        #Full rank approximation\n",
    "        if self.rk==-1:\n",
    "            self.rk=len(idx)\n",
    "                \n",
    "        qemb=self.q_emb\n",
    "        print('begin of generate kernel')\n",
    "        \n",
    "        X=self.TF_emb.toarray().transpose()\n",
    "        X=np.c_[X,qemb]\n",
    "\n",
    "        L=np.eye(X.shape[0])*np.sqrt(X.shape[0])\n",
    "        L_inv=np.linalg.inv(L)\n",
    "        G=L_inv@X@X.transpose()@L_inv\n",
    "        Gs=G[idx,:]\n",
    "        Gs=Gs[:,idx]\n",
    "        S,V,D=np.linalg.svd(Gs, full_matrices=True)\n",
    "        Ssub,Vsub=S[:,:self.rk], np.diag(V)[:self.rk,:self.rk]\n",
    "        #Ssub,Vsub=S,np.diag(V)\n",
    "        #G=Ssub@Vsub$Ssub.transpose() but this operation is not necessary, we just need the decomposition of G\n",
    "\n",
    "        D_sub_inv=np.diag(np.apply_along_axis(lambda x: 1/np.sqrt(x) , 0, V[:self.rk]))\n",
    "        #D_sub_inv=np.diag(np.apply_along_axis(lambda x: 1/np.sqrt(x) , 0, V))\n",
    "        W=(((D_sub_inv@Ssub.transpose())@X[idx,:])@(X.transpose()))@X\n",
    "        self.TF_FARAHAT_emb=np.apply_along_axis(lambda x: x/np.sqrt(np.sum(x**2)), 1,W.transpose())\n",
    "        print('end of generate kernel')\n",
    "        return self\n",
    "\n",
    "    def predict(self,metadata,repo):\n",
    "        \n",
    "        self.content_repo=self.content_doc[repo]\n",
    "        questions=self.querries['query'].tolist()\n",
    "        W_=self.TF_FARAHAT_emb\n",
    "        scores=W_[-len(questions):]@(W_[metadata[0]:metadata[1]].transpose())\n",
    "        rank=np.apply_along_axis(lambda x:np.argsort(-x),1,scores)\n",
    "        raw_or_treated=1 #raw text\n",
    "        \n",
    "        \n",
    "        all_answers=[]\n",
    "        all_scores=[]\n",
    "        all_indices=[]\n",
    "        all_models=[]\n",
    "        all_ranks=[]\n",
    "        all_querries=[]\n",
    "        self.all_answers_retrieved_raw=[]\n",
    "        self.all_answers_retrieved=[]\n",
    "        all_querries_treated=[]\n",
    "        all_question_ids=[]\n",
    "        special_answer=np.zeros_like(questions,dtype=int)\n",
    "        \n",
    "        \n",
    "        bert=BertRetriever(top_n=5)\n",
    "        for question in range(len(questions)):\n",
    "            cluster_ids=[]\n",
    "            answers=[]\n",
    "            answers_raw=[]\n",
    "            dic_answers={}\n",
    "            count_answers=1\n",
    "            #print('*****  Question {} : {} {}'.format(question,questions[question],'********'))\n",
    "            rk=0\n",
    "            count=0\n",
    "            #print('\\n----------------\\n')\n",
    "            w_important=[w for w in questions[question].split(\" \") if (w==w.upper() and len(w)>1) ]\n",
    "            #print('Important words=',w_important,rank.shape[1])\n",
    "            #Display best candidates according to clustering\n",
    "            \n",
    "            ### Special words retrieval (Tf-Idf search) ###\n",
    "            if w_important !=[]:\n",
    "                for w in w_important:\n",
    "                    try:\n",
    "                        w=TfidfRetriever(transform_text=False).tokenize(w)[0]\n",
    "                        w=w.lower()\n",
    "                        best_id=[i for i in range(rank.shape[1]) if w in self.content_repo[0][rank[question,i]].split(' ')]\n",
    "                    except:\n",
    "                        print(\"an error occured with the important word{}\".format(word))\n",
    "                    \n",
    "                    #print(\",best_id\",w,best_id)\n",
    "                    if best_id!=[]:\n",
    "                        id_match=best_id[0]\n",
    "                        result=self.content_repo[raw_or_treated][rank[question,id_match]]\n",
    "                        #print('Special Anserw : {}'.format(self.content_repo[raw_or_treated][rank[question,id_match]]))\n",
    "                        #print('\\n----------------\\n')\n",
    "\n",
    "                        all_question_ids.extend([question])\n",
    "                        all_indices.extend([rank[question,id_match]])\n",
    "                        all_scores.extend([scores[question,rank[question,id_match]]])\n",
    "                        all_answers.extend([result])\n",
    "                        all_ranks.append(rk+1)\n",
    "                        all_models.append(\"Tf_Idf_Cluster_Bert_Finetuned_MsMarco\")\n",
    "                        all_querries.append(self.querries_raw[question])\n",
    "                        rk+=1\n",
    "                        special_answer[question]+=1\n",
    "                        count_answers+=1\n",
    "            \n",
    "            ###Normal Loop Spherical Kmeans Clustering and Applying Bert encoder###\n",
    "            #1 Get clusters\n",
    "            #print('begin loop cluster retriever')\n",
    "            for answ in range(size_cluster):\n",
    "                while True:\n",
    "                    answer_raw=self.content_repo[raw_or_treated][rank[question,count]]\n",
    "                    answer=self.content_repo[1-raw_or_treated][rank[question,count]]\n",
    "                    dic_answers[answer]=dic_answers.get(answer,0)+1\n",
    "                    if dic_answers[answer]==1:\n",
    "                        break\n",
    "                    cluster_ids.append(metadata[0]+rank[question,count])\n",
    "                    count+=1\n",
    "                \n",
    "                answers.append(answer)\n",
    "                answers_raw.append(answer_raw)\n",
    "\n",
    "                #print('Anserw n°{} : {}'.format(count_answers,answer))\n",
    "                #print('\\n----------------\\n')\n",
    "            self.all_answers_retrieved.append(answers)\n",
    "            self.all_answers_retrieved_raw.append(answers_raw)\n",
    "            \n",
    "            #print('end loop cluster retriever')\n",
    "\n",
    "            #2) Use Bert encoder inside clusters + cosine similarity retrieval\n",
    "            Qst=questions[question].lower()\n",
    "            newQst=pdfconverter().remove_non_alpha(Qst)\n",
    "            newQst=newQst.replace('.','')\n",
    "            all_querries_treated.append(newQst)\n",
    "            \n",
    "        #Prediction with tensorflow app using bert finetuned on re-ranking MsMarco dataset\n",
    "        \n",
    "        o=len(all_querries_treated)\n",
    "        tokenizer = FullTokenizer(vocab_file=vocab_file, do_lower_case=True)\n",
    "        docs=[tuple(array) for array in self.all_answers_retrieved]\n",
    "        query_id=[str(i) for i in range(o)]\n",
    "        doc_ids=[tuple(array) for array in list(np.tile([str(i) for i in range(size_cluster) ],o).reshape(o,-1))]\n",
    "        labels=[0 for i in range(size_cluster)]\n",
    "        \n",
    "        \n",
    "        global max_query_length\n",
    "        global output_folder\n",
    "        \n",
    "        convert_eval_dataset(tokenizer,\n",
    "                             output_folder,\n",
    "                             output_folder + '/query_doc_ids_' + 'eval' + '.txt',\n",
    "                             max_seq_length,\n",
    "                             max_query_length,\n",
    "                             all_querries_treated,\n",
    "                             docs,\n",
    "                             labels,\n",
    "                             query_id,\n",
    "                             doc_ids)\n",
    "        \n",
    "        #print('begin reranking prediction with bert class')       \n",
    "        \n",
    "        ! python dl4marco-bert-master/run_msmarcoQA.py \\\n",
    "          --output_dir=dl4marco-bert-master/output_QA\\\n",
    "          --data_dir=dl4marco-bert-master/tfrecord \\\n",
    "          --bert_config_file=dl4marco-bert-master/data/bert_msmarco/bert_config.json \\\n",
    "          --init_checkpoint=dl4marco-bert-master/data/bert_msmarco/model.ckpt \\\n",
    "          --max_seq_length=512 \\\n",
    "          --msmarco_output=True \\\n",
    "          --iterations_per_loop=30 \\\n",
    "          --num_eval_docs=30 \\\n",
    "          --eval_batch_size=1\n",
    "        \n",
    "        #Transform tensorflow results to a nice DataFrame\n",
    "\n",
    "        ranker=pd.read_csv('dl4marco-bert-master/output_QA/msmarco_predictions_eval.tsv', sep='\\t',names=['Q_id','Doc_id','Rank','Probs'])\n",
    "        self.ranker=ranker[ranker.Rank<=5]\n",
    "        \n",
    "        for question in range(o):\n",
    "            rk=0\n",
    "            ranker_sub=self.ranker[self.ranker['Q_id']==question]\n",
    "            scores_bert=ranker_sub['Probs'].tolist()\n",
    "            indices=ranker_sub['Doc_id'].tolist()\n",
    "            text=[self.all_answers_retrieved_raw[question][i] for i in indices]\n",
    "\n",
    "            #print('end reranking prediction with bert class')\n",
    "            for i,c in enumerate(text):\n",
    "\n",
    "                #print('Anserws n° {} : {}'.format(i+1,c))\n",
    "                #print('\\n----------------\\n')\n",
    "                #print('rk=',rk+special_answer[question])\n",
    "                all_ranks.append(rk+1+special_answer[question])\n",
    "                all_models.append(\"Tf_Idf_Cluster_Bert_Finetuned_MsMarco\")\n",
    "                all_querries.append(self.querries_raw[question])\n",
    "                all_question_ids.extend([question])\n",
    "                rk+=1\n",
    "            \n",
    "            all_answers.extend(text)\n",
    "            all_scores.extend(scores_bert)\n",
    "            #print(\"ranker_sub.values\",ranker_sub.values)\n",
    "            #print(\"scores_bert\",scores_bert)\n",
    "            all_indices.extend(rank[question,:size_cluster][indices])\n",
    "        #all_indices.extend(np.array(range(len(self.content_repo[0])))[rank[question,:self.top_n]])\n",
    "            \n",
    "        final=pd.DataFrame(np.c_[all_question_ids,all_querries,all_models,all_ranks,all_indices,all_answers, all_scores],columns=['Q_ids','Question','Model','Rank','Doc_index','Answer','Score'])\n",
    "        return final.sort_values(by=['Q_ids','Rank'],ascending=True).drop(columns=['Q_ids'])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

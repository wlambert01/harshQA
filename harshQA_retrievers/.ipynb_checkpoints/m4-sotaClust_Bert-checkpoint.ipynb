{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import uuid\n",
    "import prettytable\n",
    "import time\n",
    "import cProfile\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tinyarray\n",
    "import nltk\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from tika import parser\n",
    "from nltk import tokenize as tkn\n",
    "from string import digits\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as skf\n",
    "from sklearn.base import BaseEstimator\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from models import InferSent\n",
    "import enchant\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tfidf_FarahatCluster_Bert(BaseEstimator):\n",
    "    \"\"\"\n",
    "    A scikit-learn estimator for TfidfRetriever. Trains a tf-idf matrix from a corpus\n",
    "    of documents then finds the most N similar documents of a given input document by\n",
    "    taking the dot product of the vectorized input document and the trained tf-idf matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ngram_range : bool, optional\n",
    "        [shape of ngram used to build vocab] (the default is False)\n",
    "    max_df : bool, optional\n",
    "        [while building vocab delete words that have a frequency>max_df] (the default is False)\n",
    "    stop_words : str, optional\n",
    "        ['english is the only value accepted'] (the default is False)\n",
    "    paragraphs : iterable\n",
    "        an iterable which yields either str, unicode or file objects\n",
    "    top_n : int\n",
    "        maximum number of top articles to retrieve\n",
    "        header should be of format: title, paragraphs.\n",
    "    verbose : bool, optional\n",
    "        If true, all of the warnings related to data processing will be printed.\n",
    "    Attributes\n",
    "    ----------\n",
    "    vectorizer : TfidfVectorizer\n",
    "        See https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "    tfidf_matrix : sparse matrix, [n_samples, n_features]\n",
    "        Tf-idf-weighted document-term matrix.\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> retriever = TfidfRetriever(ngram_range=(1, 2), max_df=0.85, stop_words='english')\n",
    "    >>> retriever.fit(X=df['content'])\n",
    "    \n",
    "    >>> doc_index=int(input('Which document do you want to use for your question?'))\n",
    "    >>> retriever.transform(X=df.loc[doc_index,'content'])\n",
    "    \n",
    "    >>> Q=str(input('Enter your question'))\n",
    "    >>> Q=retriever.vectorizer.transform([Q])\n",
    "    >>> closest_docs,scores = self.retriever.predict(newQst,df.loc[doc_index,'content'])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, TF_emb, q_emb, content, content_doc,stemmed_content, vocab, l_querries,l_querries_raw, by_querries=True, top_n=5,threshold_w=0.002, portion_w=1.0,rank=500,verbose=False):\n",
    "\n",
    "        self.top_n = top_n\n",
    "        self.verbose = verbose\n",
    "        self.TF_emb=TF_emb\n",
    "        self.threshold_importance=threshold_w\n",
    "        self.prop_w=portion_w\n",
    "        self.rk=rank\n",
    "        self.by_querries=by_querries\n",
    "        self.querries=l_querries.copy() #(dataframe querries with word importance)\n",
    "        self.querries_raw=l_querries_raw.copy()\n",
    "        self.content=content\n",
    "        self.content_doc=content_doc\n",
    "        self.stemmed_content=stemmed_content\n",
    "        self.vocab=vocab\n",
    "        self.dic_emb={}\n",
    "        self.q_emb=q_emb\n",
    "    \n",
    "    def select_term_Naystrom(self,T):\n",
    "\n",
    "        #T is the Tf-Idf Matrix \n",
    "        #s is the fraction of important words that we want to retrieve (s=1.0 generally)\n",
    "        print('begin of term selection')\n",
    "        #Retrieve all the stems of tf-idf vocabulary\n",
    "        terms=self.vocab\n",
    "        \n",
    "        #First we remove digits of the terms candidate\n",
    "        n=int(self.prop_w*len(terms))\n",
    "        idx_words=[]\n",
    "        for i,term in enumerate(terms):\n",
    "            try: float(term)\n",
    "            except: idx_words.append(i)\n",
    "\n",
    "        #Secondly we set a threshold to select words that appear at least each p documents\n",
    "\n",
    "        #Set threshold to select words that appear at least each p documents \n",
    "        self.freq_term=np.zeros((len(self.content),len(terms)))\n",
    "        \n",
    "        for j,stem in enumerate(self.stemmed_content):\n",
    "            for i,c in enumerate(terms):\n",
    "                if c in stem:\n",
    "                    self.freq_term[j,i]+=1\n",
    "                    \n",
    "        self.freq_term_by_doc=np.apply_along_axis(lambda x: np.mean(x),0,self.freq_term)\n",
    "        ids=np.where(self.freq_term_by_doc>self.threshold_importance )[0]\n",
    "        idx_words=[ i for i in ids if i in idx_words]\n",
    "\n",
    "\n",
    "        #Init selection of terms with most correlation \n",
    "        if self.prop_w!=1.0:\n",
    "            prob=np.sum(np.abs(T)>0,axis=0)\n",
    "            prob=prob/np.sum(prob)\n",
    "            prob=np.squeeze(prob)\n",
    "            idx=[]\n",
    "            m=len(idx_words)\n",
    "\n",
    "            for i in range(n):\n",
    "\n",
    "                p=int(np.random.choice(m,1,prob[0]))\n",
    "                idx.append(int(idx_words[p]))\n",
    "                prob[idx_words[p]]=0\n",
    "                prob=prob/np.sum(prob)\n",
    "\n",
    "            return idx,np.squeeze(T[:,idx])\n",
    "        \n",
    "        else:\n",
    "            idx=idx_words\n",
    "            self.idx=idx\n",
    "            return idx,np.squeeze(T[:,idx])\n",
    "        print('end of term selection')\n",
    "    \n",
    "    \n",
    "\n",
    "    def fit(self, X, y=None): \n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self): #generate new enhanced tf-idf-farahat features ( co-occurence weight frequency matrix) \n",
    "        \n",
    "        idx,A=self.select_term_Naystrom(self.TF_emb.toarray())   \n",
    "        \n",
    "        #Full rank approximation\n",
    "        if self.rk==-1:\n",
    "            self.rk=len(idx)\n",
    "                \n",
    "        qemb=self.q_emb\n",
    "        print('begin of generate kernel ')\n",
    "        \n",
    "        X=self.TF_emb.toarray().transpose()\n",
    "        X=np.c_[X,qemb]\n",
    "\n",
    "        L=np.eye(X.shape[0])*np.sqrt(X.shape[0])\n",
    "        L_inv=np.linalg.inv(L)\n",
    "        G=L_inv@X@X.transpose()@L_inv\n",
    "        Gs=G[idx,:]\n",
    "        Gs=Gs[:,idx]\n",
    "        S,V,D=np.linalg.svd(Gs, full_matrices=True)\n",
    "        Ssub,Vsub=S[:,:self.rk], np.diag(V)[:self.rk,:self.rk]\n",
    "        #Ssub,Vsub=S,np.diag(V)\n",
    "        #G=Ssub@Vsub$Ssub.transpose() but this operation is not necessary, we just need the decomposition of G\n",
    "\n",
    "        D_sub_inv=np.diag(np.apply_along_axis(lambda x: 1/np.sqrt(x) , 0, V[:self.rk]))\n",
    "        #D_sub_inv=np.diag(np.apply_along_axis(lambda x: 1/np.sqrt(x) , 0, V))\n",
    "        W=(((D_sub_inv@Ssub.transpose())@X[idx,:])@(X.transpose()))@X\n",
    "        self.TF_FARAHAT_emb=np.apply_along_axis(lambda x: x/np.sqrt(np.sum(x**2)), 1,W.transpose())\n",
    "        print('end of generate kernel')\n",
    "        return self\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    def predict(self,metadata,repo):\n",
    "        self.content_repo=self.content_doc[repo]\n",
    "        questions=self.querries['query'].tolist()\n",
    "        W_=self.TF_FARAHAT_emb\n",
    "        scores=W_[-len(questions):]@(W_[metadata[0]:metadata[1]].transpose())\n",
    "        rank=np.apply_along_axis(lambda x:np.argsort(-x),1,scores)\n",
    "        raw_or_treated=1 #raw text\n",
    "        \n",
    "        all_answers=[]\n",
    "        all_scores=[]\n",
    "        all_indices=[]\n",
    "        all_models=[]\n",
    "        all_ranks=[]\n",
    "        all_querries=[]\n",
    "        bert=BertRetriever(top_n=5)\n",
    "        for question in range(len(questions)):\n",
    "            cluster_ids=[]\n",
    "            answers=[]\n",
    "            answers_raw=[]\n",
    "            dic_answers={}\n",
    "            count_answers=1\n",
    "            #print('*****  Question {} : {} {}'.format(question,questions[question],'********'))\n",
    "            rk=0\n",
    "            count=0\n",
    "            #print('\\n----------------\\n')\n",
    "            w_important=[w for w in questions[question].split(\" \") if w==w.upper() ]\n",
    "            #Display best candidates according to clustering\n",
    "            \n",
    "            ### Special words retrieval (Tf-Idf search) ###\n",
    "            if w_important !=[]:\n",
    "          \n",
    "                for w in w_important:\n",
    "                    ll=TfidfRetriever().tokenize(w)\n",
    "                    #print(\"w,ll=\",w,ll)\n",
    "                    if ll!=[]:\n",
    "                        w=ll[0]\n",
    "                        best_id=[i for i in range(rank.shape[1]) if w in self.content_repo[0][rank[question,i]].split(' ')]\n",
    "                        if best_id!=[]:\n",
    "                            id_match=best_id[0]\n",
    "                            result=self.content_repo[raw_or_treated][rank[question,id_match]]\n",
    "                            #print('Special Anserw : {}'.format(self.content_repo[raw_or_treated][rank[question,id_match]]))\n",
    "                            #print('\\n----------------\\n')\n",
    "\n",
    "\n",
    "                            all_indices.extend([rank[question,id_match]])\n",
    "                            all_scores.extend([scores[question,rank[question,id_match]]])\n",
    "                            all_answers.extend([result])\n",
    "                            all_ranks.append(rk+1)\n",
    "                            all_models.append(\"Tf-Idf- Semantic Kernel- Bert\")\n",
    "                            all_querries.append(self.querries_raw[question])\n",
    "                            rk+=1\n",
    "                            count_answers+=1\n",
    "            \n",
    "            ###Normal Loop Spherical Kmeans Clustering and Applying Bert encoder###\n",
    "            #1 Get clusters\n",
    "            #print('begin retriever from cluster')\n",
    "            for answ in range(size_cluster):\n",
    "                while True:\n",
    "                    answer_raw=self.content_repo[raw_or_treated][rank[question,count]]\n",
    "                    answer=self.content_repo[1-raw_or_treated][rank[question,count]]\n",
    "                    dic_answers[answer]=dic_answers.get(answer,0)+1\n",
    "                    if dic_answers[answer]==1:\n",
    "                        break\n",
    "                    cluster_ids.append(metadata[0]+rank[question,count])\n",
    "                    count+=1\n",
    "                \n",
    "                answers.append(answer)\n",
    "                answers_raw.append(answer_raw)\n",
    "\n",
    "                #print('Anserw n°{} : {}'.format(count_answers,answer))\n",
    "                #print('\\n----------------\\n')\n",
    "                \n",
    "            #print('end loop cluster retriever')\n",
    "\n",
    "            #2) Use Bert encoder inside clusters + cosine similarity retrieval\n",
    "            Qst=questions[question].lower()\n",
    "            newQst=pdfconverter().remove_non_alpha(Qst)\n",
    "            newQst=newQst.replace('.','')\n",
    "\n",
    "            #print('initiate bert cluster retriever')\n",
    "            bert.embeddings=[]\n",
    "            bert.reduced_embeddings=[]\n",
    "            \n",
    "            \n",
    "            #embeddings = bert.transform(list([s for s in answers ]))\n",
    "            try:\n",
    "                bert.embeddings = [QAmodel.bert.embeddings[i] for i in cluster_ids]\n",
    "                bert.reduced_embeddings = np.apply_along_axis(lambda v: bert.normalize(v), 1,bert.embeddings)\n",
    "            except:\n",
    "                indices_saved=[i for (i,c) in enumerate(answers) if c in self.dic_emb.keys()]\n",
    "                print('just saved {} bert encodings'.format(len(indices_saved)))\n",
    "                indices_new=[i for i in range(len(answers)) if i not in indices_saved ]\n",
    "                indices_sort=indices_saved+indices_new\n",
    "                indices_unsort=np.argsort(indices_sort)\n",
    "                saved_embeddings=np.array([self.dic_emb[answers[i]] for i in indices_saved])\n",
    "                new_item=[answers[i] for i in indices_new]\n",
    "                if indices_new!=[]:\n",
    "                    bert.transform(list(new_item))\n",
    "                    new_embeddings=bert.reduced_embeddings\n",
    "                \n",
    "                for (i,s) in enumerate(new_item):\n",
    "                    self.dic_emb[s]=new_embeddings[i]\n",
    "                    \n",
    "                if indices_saved!=[]:\n",
    "                    if indices_new==[]:\n",
    "                        bert.reduced_embeddings=saved_embeddings\n",
    "                    else:\n",
    "                        bert.reduced_embeddings=np.r_[saved_embeddings,new_embeddings][indices_unsort]\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "            indices,scores_bert=bert.predict(newQst,[0,len(answers)])\n",
    "            scores_bert=scores_bert.loc[indices].values[:,0]\n",
    "            text=[ answers_raw[i] for i in indices]\n",
    "            for i,c in enumerate(text):\n",
    "                \n",
    "                #print('Anserws n° {} : {}'.format(i+1,c))\n",
    "                #print('\\n----------------\\n')\n",
    "                all_ranks.append(rk+1)\n",
    "                all_models.append(\"Tf-Idf- Semantic Kernel- Bert\")\n",
    "                all_querries.append(self.querries_raw[question])\n",
    "                rk+=1\n",
    "               \n",
    "            all_answers.extend(text)\n",
    "            all_scores.extend(scores_bert)\n",
    "            all_indices.extend(rank[question,:size_cluster][indices])\n",
    "            #all_indices.extend(np.array(range(len(self.content_repo[0])))[rank[question,:self.top_n]])\n",
    "            \n",
    "        return pd.DataFrame(np.c_[all_querries,all_models,all_ranks,all_indices,all_answers, all_scores],columns=['Question','Model','Rank','Doc_index','Answer','Score'])\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

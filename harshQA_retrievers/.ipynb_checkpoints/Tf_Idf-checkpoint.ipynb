{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import uuid\n",
    "import prettytable\n",
    "import time\n",
    "import cProfile\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as skf\n",
    "from sklearn.base import BaseEstimator\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from tika import parser\n",
    "from nltk import tokenize as tkn\n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfRetriever(BaseEstimator):\n",
    "    \"\"\"\n",
    "    A scikit-learn wrapper for TfidfRetriever. Trains a tf-idf matrix from a corpus\n",
    "    of documents then finds the most N similar documents of a given input document by\n",
    "    taking the dot product of the vectorized input document and the trained tf-idf matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ngram_range : bool, optional\n",
    "        [shape of ngram used to build vocab] (the default is False)\n",
    "    max_df : bool, optional\n",
    "        [while building vocab delete words that have a frequency>max_df] (the default is False)\n",
    "    stop_words : str, optional\n",
    "        ['english is the only value accepted'] (the default is False)\n",
    "    paragraphs : iterable\n",
    "        an iterable which yields either str, unicode or file objects\n",
    "    top_n : int\n",
    "        maximum number of top articles to retrieve\n",
    "        header should be of format: title, paragraphs.\n",
    "    verbose : bool, optional\n",
    "        If true, all of the warnings related to data processing will be printed.\n",
    "    Attributes\n",
    "    ----------\n",
    "    vectorizer : TfidfVectorizer\n",
    "        See https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "    tfidf_matrix : sparse matrix, [n_samples, n_features]\n",
    "        Tf-idf-weighted document-term matrix.\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> retriever = TfidfRetriever(ngram_range=(1, 2), max_df=0.85, stop_words='english')\n",
    "    >>> retriever.fit(X=df['content'])\n",
    "    \n",
    "    >>> doc_index=int(input('Which document do you want to use for your question?'))\n",
    "    >>> retriever.transform(X=df.loc[doc_index,'content'])\n",
    "    \n",
    "    >>> Q=str(input('Enter your question'))\n",
    "    >>> Q=retriever.vectorizer.transform([Q])\n",
    "    >>> closest_docs,scores = self.retriever.predict(newQst,df.loc[doc_index,'content'])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 ngram_range=(1, 2),\n",
    "                 max_df=0.85,\n",
    "                 stop_words='english',\n",
    "                 paragraphs=None,\n",
    "                 verbose=False, top_n=5,\n",
    "                lemmatize=False,\n",
    "                transform_text=True):\n",
    "\n",
    "        self.ngram_range = (1,1)\n",
    "        self.max_df = max_df\n",
    "        self.stop_words = stop_words\n",
    "        self.paragraphs = paragraphs\n",
    "        self.top_n = top_n\n",
    "        self.verbose = verbose\n",
    "        self.transform_text=transform_text\n",
    "        self.lemmatize=lemmatize and self.transform_text\n",
    "        self.stem=not lemmatize and self.transform_text\n",
    "        if self.stem: self.stemmer=PorterStemmer()\n",
    "        else: self.lemmatizer=WordNetLemmatizer() \n",
    "        self.stop_words_list=[self.tokenize(word)[0] for word in list(skf._check_stop_list('english'))]\n",
    "\n",
    "    def stem_tokens(self,tokens, stemmer):\n",
    "        stemmed = []\n",
    "        for item in tokens:\n",
    "            stemmed.append(stemmer.stem(item))\n",
    "        return stemmed\n",
    "    \n",
    "    def lemmatize_tokens(self,tokens,lemmatizer):\n",
    "        lemmas=[]\n",
    "        for item in tokens:\n",
    "            for word, tag in pos_tag(word_tokenize(item)):\n",
    "                wntag = tag[0].lower()\n",
    "                wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "                if not wntag:\n",
    "                    lemma = word\n",
    "                else:\n",
    "                    lemma = lemmatizer.lemmatize(word, wntag)\n",
    "            \n",
    "            lemmas.append(lemma)\n",
    "        return lemmas\n",
    "\n",
    "        \"\"\"\n",
    "        for item in tokens:\n",
    "            lemmas.append(lemmatizer.lemmatize(item))\n",
    "        return lemmas\n",
    "        \"\"\"\n",
    "\n",
    "    def tokenize(self,text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        if self.lemmatize:\n",
    "        #stems = self.stem_tokens(tokens, self.stemmer)\n",
    "            lemmas=self.lemmatize_tokens(tokens,self.lemmatizer)\n",
    "            return lemmas\n",
    "        elif self.stem:\n",
    "            stems = self.stem_tokens(tokens, self.stemmer)\n",
    "            return stems\n",
    "        else:\n",
    "            return tokens\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None): #generate features and return tfidf scores matrix \n",
    "\n",
    "        self.vectorizer = TfidfVectorizer(ngram_range=self.ngram_range,\n",
    "                                          max_df=self.max_df,\n",
    "                                   stop_words=self.stop_words_list,tokenizer=self.tokenize)\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        self.tfidf_matrix=self.vectorizer.transform(X)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, metadata):\n",
    "        tfidf_matrix=self.tfidf_matrix[metadata[0]:metadata[1],:]\n",
    "        #cherche les querries les plus proches de chaque sentence\n",
    "        t0 = time.time()\n",
    "        question_vector = self.vectorizer.transform([X])\n",
    "        scores = pd.DataFrame(tfidf_matrix.dot(question_vector.T).toarray())\n",
    "        closest_docs_indices = scores.sort_values(by=0, ascending=False).index[:self.top_n].values\n",
    "\n",
    "        # inspired from https://github.com/facebookresearch/DrQA/blob/50d0e49bb77fe0c6e881efb4b6fe2e61d3f92509/scripts/reader/interactive.py#L63\n",
    "        if self.verbose:\n",
    "            rank = 1\n",
    "            table = prettytable.PrettyTable(['rank', 'index', 'title'])\n",
    "            for i in range(len(closest_docs_indices)):\n",
    "                index = closest_docs_indices[i]\n",
    "                if self.paragraphs:\n",
    "                    article_index = self.paragraphs[int(index)]['index']\n",
    "                    title = metadata.iloc[int(article_index)]['title']\n",
    "                else:\n",
    "                    title = metadata.iloc[int(index)]['title']\n",
    "                table.add_row([rank, index, title])\n",
    "                rank+=1\n",
    "            print(table)\n",
    "            print('Time: {} seconds'.format(round(time.time() - t0, 5)))\n",
    "\n",
    "        return closest_docs_indices,scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

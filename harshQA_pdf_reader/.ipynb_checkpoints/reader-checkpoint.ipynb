{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import uuid\n",
    "import prettytable\n",
    "import time\n",
    "import cProfile\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\"\"\" If you never installed punkt and wordnet:\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')\n",
    "!python -m textblob.download_corpora\n",
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "from tika import parser\n",
    "from nltk import tokenize as tkn\n",
    "from string import digits\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from textblob import TextBlob\n",
    "import enchant\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pdfconverter():\n",
    "    def __init__(self,directory_path=None):\n",
    "        global domain\n",
    "        global retrieved_company\n",
    "        directory_path=pdf_directory+domain+'/'\n",
    "        \n",
    "        self.english_voc=enchant.Dict(\"en_US\")\n",
    "        self.text_processor_pdf=np.vectorize(self.text_preprocessing_pdf,otypes=[str])\n",
    "        self.df = pd.DataFrame(columns=['pdf','directory','directory_index','raw paragraphs','paragraphs'])\n",
    "        self.parser=[]\n",
    "        self.parser_raw=[]\n",
    "        self.directory_path=directory_path\n",
    "\n",
    "        self.list_folder=[]\n",
    "        self.paths={}\n",
    "        directories=[directory_path+retrieved_company, directory_path+'Domain_vocab']\n",
    "        for dirs in directories:\n",
    "            for r,d,f in os.walk(dirs):\n",
    "                if d==[] and 'pdf' in '.'.join(f):\n",
    "                    self.list_folder.append(r)\n",
    "\n",
    "            for folder in self.list_folder:\n",
    "                for i,pdf in enumerate(os.listdir(folder)):\n",
    "                    if pdf!= '.DS_Store':\n",
    "                        self.paths[folder]=self.paths.get(folder,[])+[(i,pdf)]\n",
    "        \n",
    "    def transform(self):\n",
    "        print('************ READER *************')\n",
    "        print(\"Reading pdfs doc on location: \",pdf_directory+domain+'/'+retrieved_company+'/pdfs/')\n",
    "        \"\"\"Pdf-files reader with Apache Tika\"\"\"\n",
    "        count=1\n",
    "        for i,folder in enumerate(self.list_folder):\n",
    "            path=folder\n",
    "            for j,pdf in enumerate(os.listdir(path)):\n",
    "                if pdf!= '.DS_Store':\n",
    "                    self.df.loc[count] = [pdf,folder.split('/')[-2], i+1,None,None]\n",
    "                    \n",
    "                    \"\"\" 0- Read Pdf file \"\"\"\n",
    "                    raw = parser.from_file(os.path.join(path,pdf))\n",
    "                    s = raw['content']\n",
    "                    \n",
    "                    \"\"\" 1- Handle linebreaks to optimize TextBlob.sentences results\"\"\"\n",
    "                    s=self.treat_new_line(s)\n",
    "                    \n",
    "                    \"\"\" 2- Divide text by sentences using TextBlob\"\"\"\n",
    "                    blob=TextBlob(s)\n",
    "                    paragraphs = np.array([str(s) for s in blob.sentences],dtype=str)\n",
    "                    self.parser = []\n",
    "                    self.parser_raw=[]\n",
    "                    p=self.text_processor_pdf(paragraphs)\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    3- Get rid of bad text data:\n",
    "                    Discard sentences with too long word (16 is the 99% quantile in english)\n",
    "                    Discard sentences with too much upper words (CREDENTIALS, Link, TITLE ..)\n",
    "                    \"\"\"\n",
    "                    index_=[i for i,c in enumerate(self.parser) if (True in [len(w)>=16 for w in c.split()] )]\n",
    "                    index_raw=[i for i,c in enumerate(self.parser_raw) if np.sum([w==w.upper() for w in c.split()])>=4]\n",
    "                    index=list(set(index_ + index_raw))\n",
    "                    self.df.loc[count,'paragraphs']=np.delete(np.array(self.parser),index)\n",
    "                    self.df.loc[count,'raw paragraphs']=np.delete(np.array(self.parser_raw),index)\n",
    "                    count+=1\n",
    "                            \n",
    "            print(\"files from {} succesfully converted \".format(folder))\n",
    "                \n",
    "        return self.df\n",
    "    \n",
    "    def remove_non_alpha(self,text):\n",
    "        \n",
    "        \"\"\" Remove non alpha-decimal caracters that are not dot or linebreaker \"\"\"\n",
    "        \n",
    "        removelist=\"-\\.\\/\\?\"\n",
    "        re_alpha_numeric1=r\"[^0-9a-zA-Z\"+removelist+\" ]\"\n",
    "        clean_text=re.sub(re_alpha_numeric1,'',text)\n",
    "        clean_text=clean_text.replace('/',' ')\n",
    "        clean_text=re.sub(' +', ' ', clean_text)\n",
    "        return clean_text\n",
    "    \n",
    "    def treat_new_line(self,text):\n",
    "        \"\"\" \n",
    "        This function is aimed to deal with all types of linebreaks we met during our tests \n",
    "        There is linebreaks dure to cut-sentences, cut-words, bullet-list, title, new paragraphs, or sentences breaks\n",
    "        \"\"\"\n",
    "        text=text.replace('.\\n','. ')\n",
    "        text=re.sub(r'(\\n\\s*)+\\n+', '\\n\\n',text )\n",
    "        \n",
    "        lw=text.split('\\n\\n')\n",
    "        lw=[c for c in lw if c.replace(' ','')!='']\n",
    "            \n",
    "        for i in range(1,len(lw)):\n",
    "            try:\n",
    "\n",
    "                el=lw[i]\n",
    "                if len(el)>=1:\n",
    "                    try:\n",
    "                        first_w=el.split()[0]\n",
    "                    except:\n",
    "                        first_w=el\n",
    "                    first_l=first_w[0]\n",
    "                    if first_l.isupper() :\n",
    "                        if len(lw[i-1])>0 and lw[i-1].replace(' ','') !='':\n",
    "                            if lw[i-1].replace(' ','')[-1] not in [\":\",'.',\"-\",'/',\"'\",\";\"]:\n",
    "                                prec=lw[i-1].split(\".\")[-1]\n",
    "                                merge=(prec+' '+lw[i]).split()\n",
    "                                dic=dict(nltk.tag.pos_tag(merge))\n",
    "                                proper_noun=dic[first_w]=='NNP'\n",
    "                                if not proper_noun:\n",
    "                                    if not \".\" in lw[i-1]:\n",
    "                                        lw[i-1]=lw[i-1]+\".\\n\\n \"\n",
    "                                    else:\n",
    "                                        lw[i-1]=lw[i-1][:-1]+\".\\n\\n \"\n",
    "                                else:\n",
    "                                    lw[i-1]+=' '\n",
    "\n",
    "\n",
    "                    elif first_l.islower():\n",
    "                        if len(lw[i-1])>0 and lw[i-1][-1].replace(' ','')!='':\n",
    "\n",
    "                            if lw[i-1][-1].replace(' ','')[-1]!='-':\n",
    "                                lw[i-1]+=\"\"\n",
    "                            else:\n",
    "\n",
    "                                ltemp_prev=lw[i-1].split(' ')\n",
    "                                ltemp_next=lw[i].split(' ')\n",
    "                                motprev=ltemp_prev[-1][:-1]\n",
    "                                motnext=lw[i].split(' ')[0]\n",
    "                                if len((motprev+' '+motnext).split())==2:\n",
    "\n",
    "                                    if self.english_voc.check(motprev) and self.english_voc.check(motnext) and not self.english_voc.check(\"\".join([motprev,motnext])) :\n",
    "                                        newmot=\" \".join([motprev,motnext])\n",
    "                                    else:\n",
    "                                        newmot=\"\".join([motprev,motnext])\n",
    "                                    ltemp_prev[-1]=newmot\n",
    "                                    ltemp_next[0]=\"\"\n",
    "                                    lw[i-1]=\" \".join(ltemp_prev)\n",
    "                                    lw[i]=\" \".join(ltemp_next)\n",
    "                    else:\n",
    "                        lw[i-1]+=\"\\n\\n\"\n",
    "            \n",
    "            except:\n",
    "                print('Error occurs, the reader may not be suitable for your pdf files')\n",
    "            \n",
    "            \n",
    "        text=\"\".join(lw)\n",
    "        \n",
    "        lw=text.split('\\n')\n",
    "        lw=[c for c in lw if c.replace(' ','')!='']\n",
    "        for i in range(1,len(lw)):\n",
    "            try:\n",
    "                el=lw[i]\n",
    "                if len(el)>=1:\n",
    "                    try:\n",
    "                        first_w=el.split()[0]\n",
    "                    except:\n",
    "                        first_w=el\n",
    "                    first_l=first_w[0]\n",
    "                    if first_l.isupper() :\n",
    "                        if len(lw[i-1])>0 and lw[i-1].replace(' ','')!='':\n",
    "                            if lw[i-1].replace(' ','')[-1] not in [\":\",'.',\"-\",'/',\"'\",\";\"]:\n",
    "                                prec=lw[i-1].split(\".\")[-1]\n",
    "                                merge=(prec+' '+lw[i]).split()\n",
    "                                dic=dict(nltk.tag.pos_tag(merge))\n",
    "                                proper_noun=dic[first_w]=='NNP'\n",
    "                                if not proper_noun:\n",
    "                                    if not \".\" in lw[i-1]:\n",
    "                                        lw[i-1]=lw[i-1]+\".\\n\\n \"\n",
    "                                    else:\n",
    "                                        lw[i-1]=lw[i-1][:-1]+\".\\n\\n \"\n",
    "                                else:\n",
    "                                    lw[i-1]+=' '\n",
    "                    elif first_l.islower():\n",
    "                        if len(lw[i-1])>0 and lw[i-1].replace(' ','')!='':\n",
    "                            if lw[i-1].replace(' ','')[-1]==\"-\":\n",
    "                                ltemp_prev=lw[i-1].split(' ')\n",
    "                                ltemp_next=lw[i].split(' ')\n",
    "                                motprev=ltemp_prev[-1][:-1]\n",
    "                                motnext=lw[i].split(' ')[0]\n",
    "                                if len((motprev+' '+motnext).split())==2:\n",
    "                                    if self.english_voc.check(motprev) and self.english_voc.check(motnext) and not self.english_voc.check(\"\".join([motprev,motnext])) :\n",
    "                                        newmot=\" \".join([motprev,motnext])\n",
    "                                    else:\n",
    "                                        newmot=\"\".join([motprev,motnext])\n",
    "                                    ltemp_prev[-1]=newmot\n",
    "                                    ltemp_next[0]=\"\"\n",
    "                                    lw[i-1]=\" \".join(ltemp_prev)\n",
    "                                    lw[i]=\" \".join(ltemp_next)\n",
    "\n",
    "\n",
    "\n",
    "                            else:\n",
    "                                lw[i-1]+=\" \"\n",
    "                    else:\n",
    "                        lw[i-1]+=\" \"\n",
    "        \n",
    "            except:\n",
    "                print('Error occurs, the reader may not be suitable for your pdf files')\n",
    "        \n",
    "        text=\"\".join(lw)\n",
    "        return text\n",
    "    \n",
    "    \"\"\"\n",
    "    def remove_end_paragraphs(self,p):\n",
    "        if '-\\n' in p:\n",
    "            paraph=[]\n",
    "            ltemp=p.split(' ')\n",
    "            for mot in ltemp:\n",
    "                if '-\\n' in mot:\n",
    "                    if len(mot.replace('-\\n',' ').split())==2:\n",
    "                        mot1,mot2=mot.replace('-\\n',' ').split()\n",
    "                        if self.english_voc.check(mot1) and self.english_voc.check(mot2) and not self.english_voc.check(\"\".join([mot1,mot2])) :\n",
    "                            newmot=\" \".join([mot1,mot2])\n",
    "                        else:\n",
    "                            newmot=\"\".join([mot1,mot2])\n",
    "                        paraph.append(newmot)\n",
    "                else:\n",
    "                    paraph.append(mot)\n",
    "            p=\" \".join(paraph)\n",
    "        return p.replace('\\n',' ')\n",
    "    \"\"\"\n",
    "    \n",
    "    def cut_text(self,p):\n",
    "        \n",
    "        \"\"\" Cut text into sentences \"\"\"\n",
    "        if \"?\" not in p and len(p)>=100:\n",
    "            \n",
    "            phrases=self.remove_non_alpha(p)    \n",
    "            phrases=phrases.replace('.',' ')\n",
    "            phrases=phrases.replace('-',' ')\n",
    "            phrases=phrases.replace(\"?\",\" \")\n",
    "            phrases=re.sub(' +', ' ', phrases)\n",
    "            phrases=re.sub(r'([0-9]+(?=[a-z])|(?<=[a-z])[0-9]+)',\"\",phrases)\n",
    "            phrases=phrases.lower()\n",
    "            self.parser.append(re.sub(' +', ' ', phrases))\n",
    "            \n",
    "        return None \n",
    "    \n",
    "    def cut_text_raw(self,p):\n",
    "        \"\"\"Cut raw/untreated text into sentences \"\"\"\n",
    "        \n",
    "        if \"?\" not in p and len(self.remove_non_alpha(p))>=100:\n",
    "            self.parser_raw.append(re.sub(' +', ' ', p))\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def text_preprocessing_pdf(self,p):\n",
    "        \"\"\" Pipeline of sentences-preprocessing using np.vectorize for faster results \"\"\"\n",
    "        #remover_end_paragraphs=np.vectorize(self.remove_end_paragraphs,otypes=[str])\n",
    "        cleaner=np.vectorize(self.remove_non_alpha,otypes=[str])\n",
    "        cut_text=np.vectorize(self.cut_text,otypes=[str])\n",
    "        cut_text_raw=np.vectorize(self.cut_text_raw,otypes=[str])\n",
    "        cut_text_raw(p)\n",
    "        p=cleaner(p)\n",
    "        cut_text(p)\n",
    "        return p\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

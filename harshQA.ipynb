{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Question Answering -  ESG Assessment Projects BNP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">PARTIE CODE -------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of content \n",
    "\n",
    "- [Code dependencies of the project](#Code-dependencies-of-the-project)\n",
    "- [Args definition](#Arguments)\n",
    "- [Question Answering Pipeline](#QA-Pipeline)\n",
    "- [Main](#Main-program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code dependencies of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Pipeline for Question Answering on closed domain and non factoid questions - harshQA\n",
    "## Developped by William Lambert  (Risk AIR Team , BNP Paribas)\n",
    "\n",
    "import warnings\n",
    "from utils.utils  import hide_warn\n",
    "warnings.warn=hide_warn\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import uuid\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from string import digits\n",
    "from sklearn.base import BaseEstimator\n",
    "from tabulate import tabulate\n",
    "\n",
    "#Bert_dependencies\n",
    "from harshQA_reranker.tokenization import* \n",
    "import harshQA_reranker.metrics as metrics\n",
    "import harshQA_reranker.modeling as modeling\n",
    "import harshQA_reranker.optimization as optimization \n",
    "\n",
    "#Import our pdf reader\n",
    "from harshQA_pdf_reader.reader import pdfconverter\n",
    "\n",
    "#Import bert finetuned pipeline\n",
    "from harshQA_reranker.harshQA_tfrecord import *\n",
    "from harshQA_reranker.harshQA_bert_builder import * \n",
    "from harshQA_reranker.harshQA_run_msmarco import run_msmarco\n",
    "\n",
    "#Import all our models, wrap in a scikit learn estimator\n",
    "from harshQA_retrievers.m1__Infersent import m1_Infersent\n",
    "from harshQA_retrievers.m2__Bert import m2_Bert\n",
    "from harshQA_retrievers.m3__Tfidf import m3_Tfidf\n",
    "from harshQA_retrievers.m5__harshQA import m5_harshQA\n",
    "\n",
    "#Utils\n",
    "from utils.utils import remove_non_alpha\n",
    "from utils.utils import generate_querries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments[\"model\"]=5\n",
    "arguments[\"demo\"]=True\n",
    "arguments[\"demo_query\"]='Does the company support local agriculture'\n",
    "arguments[\"demo_topics\"]='local agriculture'\n",
    "arguments[\"top_n\"]=5\n",
    "arguments[\"query_dir\"]='./data/pdf_files/Tourism/Queries.txt'\n",
    "arguments[\"size_cluster\"]=80\n",
    "arguments[\"domain\"]=\"Tourism\"\n",
    "arguments[\"retrieved_company\"]=\"Disney\"\n",
    "arguments[\"pdf_directory\"]=\"./data/pdf_files/\"\n",
    "arguments[\"vocab_file\"]=\"./data/bert/pretrained_models/uncased_L-12_H-768_A-12/vocab.txt\"\n",
    "arguments[\"whole_corpus\"]=\"./data/pdf_files/All\"\n",
    "arguments[\"vocab_builder\"]=\"./data/corpusESG.json\"\n",
    "arguments[\"w2v_path\"]=\"./data/fastText/crawl-300d-2M.vec\"\n",
    "arguments[\"model_path\"]=\"./data/encoder/infersent2.pkl\"\n",
    "arguments[\"output_dir\"]=\"./data/output\"\n",
    "arguments[\"bert_config_file\"]=\"./data/bert_msmarco/bert_config.json\"\n",
    "arguments[\"init_checkpoint\"]=\"./data/bert_msmarco/model.ckpt\"\n",
    "arguments[\"max_seq_length\"]=512\n",
    "arguments[\"max_query_length\"]=128\n",
    "arguments[\"msmarco_output\"]=True\n",
    "arguments[\"do_train\"]=False\n",
    "arguments[\"do_eval\"]=True\n",
    "arguments[\"train_batch_size\"]=200\n",
    "arguments[\"eval_batch_size\"]=40\n",
    "arguments[\"learning_rate\"]=1e-6\n",
    "arguments[\"num_train_steps\"]=400000\n",
    "arguments[\"max_eval_examples\"]=None\n",
    "arguments[\"num_warmup_steps\"]=40000\n",
    "arguments[\"save_checkpoints_steps\"]=100\n",
    "arguments[\"iterations_per_loop\"]=10\n",
    "arguments[\"min_gram\"]=1\n",
    "arguments[\"max_gram\"]=1\n",
    "arguments[\"lemmatize\"]=False\n",
    "arguments[\"transform_text\"]=True\n",
    "arguments[\"sentences_chunk\"]=1\n",
    "arguments['use_tpu']=False\n",
    "arguments[\"tpu_name\"]=None\n",
    "arguments[\"tpu_zone\"]=None\n",
    "arguments[\"gcp_project\"]=None\n",
    "arguments[\"master\"]=None\n",
    "arguments[\"num_tpu_cores\"]=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_args():\n",
    "    \n",
    "    assert tf.__version__[0]=='1' , \"This code has been implemented on tf 1.14, if you want to use you should modify the code\"\n",
    "    assert arguments['retrieved_company']!=None, \"Select a Company\"\n",
    "    assert arguments['domain']!=None, \"Select a Domain\"\n",
    "    assert arguments['model']!=None, \"Select a Model\"\n",
    "    assert arguments['pdf_directory']!=None, \"Select a $PATH which contains the folder Domain/Company/pdfs/ containing pdf files to query\"\n",
    "    assert arguments['whole_corpus']!=None, \"Select a pdf directory path which contains all the pdf of the corpus to fit our model\"\n",
    "    assert arguments['vocab_builder']!=None, \"Enter a .json path to save all our vocabulary while ingesting pdf files \"\n",
    "    assert arguments['lemmatize']==False or arguments['model']==3 ,\"Lemmatize option is only available for Tf-Idf model (model n°3)\"\n",
    "    assert arguments['max_gram']==1 or arguments['model']==3 , \"Multi gram option is only available for Tf-Idf model (model n°3)\"\n",
    "    assert arguments['transform_text']==True, \"Your model will work better with a tokenizer that used (stemming/lemmatizing)\"\n",
    "    \n",
    "    if arguments['model']==5:\n",
    "        assert arguments['eval_batch_size']<=arguments['size_cluster'],\"eval batch size should be less than the size of the cluster of preselected sentences\"\n",
    "        assert arguments['size_cluster']%arguments['eval_batch_size']==0,\"eval batch size should be a multiple of the size of the cluster of preselected sentences\"\n",
    "        assert arguments['bert_config_file']!=None, \"Enter a .json bert config file to specify the model architecture\"\n",
    "        assert arguments['vocab_file']!=None, \"Enter the path of uncased_L-12_H-768_A-12/vocab.txt\"\n",
    "        assert \"uncased_L-12_H-768_A-12\" in arguments['vocab_file'], \"You need to pass the vocab file of bert uncased L-12_H-768_A-12 \"\n",
    "        assert \".txt\" in arguments['vocab_file'], \"The bert vocab_file must be a .txt file\"\n",
    "        assert \".json\" in arguments['bert_config_file'], \"The bert_config_file must be a .json file\"\n",
    "        assert arguments['output_dir']!=None, \"Enter the output directory where all the model bert,tfidf checkpoints will be written after train \"\n",
    "        \"It will also store the raw tsv files and the tfrecords used to feed bert-reranker.\"\n",
    "        assert arguments['init_checkpoint']!=None,\"Enter a bert .ckpt init checkpoint\"\n",
    "        assert \".ckpt\" in arguments['init_checkpoint'], \"The init_checkpoint must be a .ckpt file\"\n",
    "        \n",
    "    \n",
    "        \n",
    "    if arguments['model'] in [1,5]:\n",
    "        assert arguments['w2v_path']!=None,\"Specify the .vec file path of GloVe or fasText\"\n",
    "        assert \".vec\" in arguments['w2v_path'], \"The w2v path of GloVe or fasText muste be a .vec file\"\n",
    "        assert arguments['model_path']!=None,\"Specify the .pkl file path of Infersent2\"\n",
    "        assert \".pkl\" in arguments['model_path'], \"The infersent model file muste be a .pkl file\"\n",
    "            \n",
    "    \n",
    "    if arguments['demo']:\n",
    "        assert arguments['demo_query']!=None, \"Specify a query for the demo\"\n",
    "        if arguments['model']==5:\n",
    "            assert arguments['demo_topics']!=None, \"Specify coma separated topics linked to your query for the demo\"\n",
    "    else:\n",
    "        assert arguments['query_path']!=None , 'Specify a .txt file containing your queries line by line'\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QApipeline():\n",
    "    \n",
    "    def __init__(self,**kwargs):\n",
    "\n",
    "\n",
    "            #new kwargs: 'threshold' (float between 0.5 and 1.0)\n",
    "\n",
    "        self.kwargs_converter = {key: value for key, value in kwargs.items()\n",
    "                                if key in pdfconverter.__init__.__code__.co_varnames}\n",
    "\n",
    "        self.kwargs_Tf_Idf = {key: value for key, value in kwargs.items()\n",
    "                             if key in m3_Tfidf.__init__.__code__.co_varnames}\n",
    "\n",
    "        self.kwargs_Infersent={key: value for key, value in kwargs.items()\n",
    "                             if key in m1_Infersent.__init__.__code__.co_varnames}\n",
    "        self.kwargs_Bert={key: value for key, value in kwargs.items()\n",
    "                             if key in m2_Bert.__init__.__code__.co_varnames}\n",
    "\n",
    "\n",
    "        self.MODELS=['INFERSENT - GLOVE','BERT PRETRAINED','TFIDF - LEMMATIZER & BIGRAM','BERT & TFIDF SHORT TEXT CLUSTERING','BERT FINETUNED & TFIDF SHORT TEXT CLUSTERING']\n",
    "        self.usemodel=arguments['model']\n",
    "        self.sentences_chunk=kwargs['sentences_chunk']\n",
    "        return None\n",
    "\n",
    "\n",
    "    def fit_reader(self,df=None):\n",
    "\n",
    "        #CALL PDF READER \n",
    "        if df!=None:\n",
    "            assert not False in [ col in df.columns.tolist() for col in ['directory_index','raw_paragraphs','paragraphs']], \"The given dataframe is not of the proper format \"\n",
    "            self.df=df\n",
    "        else:\n",
    "            \n",
    "            print('{}*********** READER *************'.format('\\n'))\n",
    "            print(\"Reading pdfs doc on location: {}\".format(arguments['pdf_directory']+arguments['domain']+'/'+arguments['retrieved_company']+'/pdfs/'))\n",
    "            self.df=pdfconverter(**self.kwargs_converter).transform()\n",
    "\n",
    "\n",
    "            #print('You should give either your own harshQA dataframe to the fit_reader module or specify your pdf_directories, domain and retrieved_company FLAGS')\n",
    "\n",
    "\n",
    "\n",
    "        #BUILD CONTENT AND DOCUMENT INDEX \n",
    "        self.content=[]\n",
    "        self.content_raw=[]\n",
    "        self.contents_doc=[]\n",
    "        self.borders=[0]\n",
    "\n",
    "        print('********* DOCUMENTS RETRIEVED **********')\n",
    "        for j,repo in enumerate(sorted(list(set(self.df.directory_index)))):\n",
    "\n",
    "            count_dic=[{},{}]\n",
    "            remove_idx=[[],[]]\n",
    "            content_doc=[]\n",
    "            content_doc_raw=[]\n",
    "\n",
    "            title=self.df[self.df.directory_index==repo].directory.tolist()[0]\n",
    "            self.df[self.df.directory_index==repo]['raw_paragraphs'].apply(lambda sentences: self.update_count_dic(sentences,count_dic,0,remove_idx))\n",
    "            self.df[self.df.directory_index==repo]['paragraphs'].apply(lambda sentences: self.update_count_dic(sentences,count_dic,1,remove_idx))\n",
    "            self.df[self.df.directory_index==repo]['raw_paragraphs'].apply(lambda sentences: content_doc_raw.extend(sentences))\n",
    "            self.df[self.df.directory_index==repo]['paragraphs'].apply(lambda sentences: content_doc.extend(sentences))\n",
    "\n",
    "            #REMOVE TWIN SENTENCES AND REMOVE TOO SMALL SENTENCES\n",
    "            remove_idx=list(set(remove_idx[0]+remove_idx[1]))\n",
    "            content_doc=np.delete(np.array(content_doc),remove_idx)\n",
    "            content_doc_raw=np.delete(np.array(content_doc_raw),remove_idx)\n",
    "\n",
    "            content=[content_doc[i] for i in range(len(content_doc)) if (len(content_doc[i])>=50 )]\n",
    "            content_raw=[content_doc_raw[i] for i in range(len(content_doc)) if (len(content_doc[i])>=50)]\n",
    "            self.borders.append(len(content))\n",
    "\n",
    "            print(\"FOLDER : {} , {} sentences \\n \\n\".format(self.df.directory_index.unique()[j],len(content)))\n",
    "\n",
    "            #ADD SENTENCES TO OUR FINAL OBJECTS\n",
    "            self.content.extend(list(content))\n",
    "            self.content_raw.extend(list(content_raw))\n",
    "            self.contents_doc.append([content,content_raw])\n",
    "\n",
    "\n",
    "        self.borders=list(np.cumsum(self.borders))\n",
    "\n",
    "        #GROUP SENTENCES BY PAIR EVENTUALLY\n",
    "        if self.sentences_chunk==2:\n",
    "\n",
    "            self.content=[ ' '.join(x) for x in zip(self.content[0::2], self.content[1::2]) ]\n",
    "            self.content_raw=[ ' '.join(x) for x in zip(self.content_raw[0::2], self.content_raw[1::2]) ]\n",
    "            for i,(treated_sentences,raw_sentences) in enumerate(self.contents_doc):\n",
    "                self.contents_doc[i][0]=[ ' '.join(x) for x in zip(treated_sentences[0::2], treated_sentences[1::2]) ]\n",
    "                self.contents_doc[i][1]=[ ' '.join(x) for x in zip(raw_sentences[0::2], raw_sentences[1::2]) ]\n",
    "            self.borders=[int(i/2) for i in self.borders]\n",
    "\n",
    "        #REPLACE ALL DIGITS WITH SPECIAL TOKEN FOR OUR MODEL\n",
    "        for i,c in enumerate(self.borders[:-1]):\n",
    "            start_idx=self.borders[i]\n",
    "            content=self.contents_doc[i][0]\n",
    "            content_raw=self.contents_doc[i][1]\n",
    "\n",
    "            #ADD TREATED TEXT TO CONTENTS_DOC\n",
    "            for sentence_id,sentence in enumerate(content):\n",
    "\n",
    "                words_list=sentence.split(\" \")\n",
    "                for word_id,w in enumerate(words_list):\n",
    "                    try:\n",
    "                        float(w)\n",
    "                        words_list[word_id]=\"XXX\"\n",
    "                    except:\n",
    "                        words_list[word_id]=w\n",
    "                    self.content[start_idx+sentence_id]=\" \".join(words_list)\n",
    "\n",
    "                self.contents_doc[i][0][sentence_id]=\" \".join(words_list)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit(self):\n",
    "\n",
    "\n",
    "        print('********* MODEL {} **********'.format(self.MODELS[self.usemodel-1]))\n",
    "\n",
    "        if self.usemodel==1:\n",
    "            #Fit Infersent\n",
    "            self.model_retriever = m1_Infersent(**self.kwargs_Infersent)\n",
    "            self.model_retriever.fit(self.content)\n",
    "            self.model_retriever.transform(self.contents_doc[0][0])\n",
    "\n",
    "        if self.usemodel==2:\n",
    "            #Fit Bert pretrained\n",
    "\n",
    "            self.model_retriever=m2_Bert(**self.kwargs_Bert)\n",
    "            self.model_retriever.fit(self.content)#no finetuning for Bert\n",
    "            self.model_retriever.transform(self.contents_doc[0][0])\n",
    "\n",
    "        if self.usemodel==3:\n",
    "            #Fit Tf-Idf model\n",
    "\n",
    "            \"\"\"\n",
    "            This model is also called by HarshQA model (m5), so we need to turn off the saving output feature so that \n",
    "            it does not erase the harshQA output.\n",
    "            \"\"\"\n",
    "            self.kwargs_Tf_Idf['save_idfs_path']=None\n",
    "            self.kwargs_Tf_Idf['save_features_path']=None\n",
    "\n",
    "\n",
    "            self.model_retriever = m3_Tfidf(**self.kwargs_Tf_Idf)\n",
    "            self.model_retriever.fit(self.content)\n",
    "            self.model_retriever.transform(self.contents_doc[0][0])\n",
    "\n",
    "\n",
    "\n",
    "        if self.usemodel==5:\n",
    "            #Fit harshQA model \n",
    "\n",
    "            output_TF=arguments['output_dir']+'/tf_idf_checkpoints/'\n",
    "\n",
    "            args_harshQA={'save_kernel_path':output_TF+'kernel.npy',\n",
    "                          'save_kernel_vocab_path':output_TF+'kernel_vocab.json',\n",
    "                          'save_kernel_idx_path':output_TF+'kernel_vocab_idx.json',\n",
    "                          'save_idfs_path':output_TF+'idfs.npy',\n",
    "                          'save_features_path':output_TF+'vocab.json'}\n",
    "\n",
    "            for key, value in arguments.items():\n",
    "                if key in m5_harshQA.__init__.__code__.co_varnames:\n",
    "                    args_harshQA[key]=value\n",
    "\n",
    "            self.model_retriever=m5_harshQA(**args_harshQA)\n",
    "            self.model_retriever.fit(self.content)\n",
    "            self.model_retriever.transform(self.contents_doc[0])\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "        #Initialisation of Tf-Idf-Farahat\n",
    "\n",
    "    def predict(self,Qst,Topics=None):\n",
    "        \"\"\"\n",
    "        kwargs:\n",
    "        ##VE_type: 'DP' for Detect Presence of 'VE' for Value extraction\n",
    "        ##Qst: Querry\n",
    "        ##VE_cdt : null\n",
    "        \"\"\"\n",
    "\n",
    "        repo_to_query=0\n",
    "        self.Qst_raw=Qst\n",
    "        self.topics=Topics\n",
    "\n",
    "        #Apply corpus transformations to querry before feeding it into our models\n",
    "        newQst=[q.lower() for q in self.Qst_raw]\n",
    "        newQst=[remove_non_alpha(q) for q in newQst]\n",
    "        newQst=[q.replace('.','') for q in newQst]\n",
    "\n",
    "        self.dataframe=[]        \n",
    "        all_scores=[]\n",
    "        all_models=[]\n",
    "        all_querries=[]\n",
    "        all_ranks=[]\n",
    "        all_indices=[]\n",
    "        all_answers=[]\n",
    "\n",
    "        #Infersent retriever\n",
    "        if self.usemodel !=5:\n",
    "            for i,qu in enumerate(newQst):\n",
    "\n",
    "                indices,scores=self.model_retriever.predict(qu)\n",
    "                p=len(indices)\n",
    "                all_scores.extend(scores.loc[indices].values[:,0])\n",
    "                all_answers.extend([ self.contents_doc[repo_to_query][1][i] for i in indices])\n",
    "                all_models.extend([self.MODELS[arguments['model']-1]]*p)\n",
    "                all_ranks.extend(list(range(1,p+1)))\n",
    "                all_querries.extend([self.Qst_raw[i]]*p)\n",
    "                all_indices.extend(indices)\n",
    "\n",
    "\n",
    "            self.dataframe=pd.DataFrame(np.c_[all_querries,all_models,all_ranks,all_indices,all_answers,all_scores],columns=['Question','Model','Rank','Doc_index','Answer','Score'])\n",
    "\n",
    "\n",
    "        #harshQA retriever\n",
    "        else:\n",
    "            self.dataframe=self.model_retriever.predict(self.Qst_raw,self.topics)\n",
    "\n",
    "\n",
    "        #FORMAT THE OUTPUT NICELY AND RETURN IT\n",
    "        self.dataframe=self.dataframe.apply(self.add_ctxt,axis=1)\n",
    "        self.dataframe['Rank']=self.dataframe['Rank'].map(lambda x: x[0])\n",
    "        self.dataframe['Score']=self.dataframe['Score'].map(lambda x: np.round(float(x),4))\n",
    "        self.dataframe['Company']=[arguments['retrieved_company']]*len(self.dataframe)\n",
    "        self.dataframe=self.dataframe.sort_values(by=['Question','Company','Model','Rank']).reset_index(drop=True)[['Question','Company','Model','Answer','Rank','Score','Doc_index','Context_Answer']]\n",
    "        return self.dataframe\n",
    "\n",
    "\n",
    "    def string_retriever(self,sentence_list):\n",
    "        return [w  for w in sentence_list if not w.isdigit()]\n",
    "\n",
    "    def add_ctxt(self,row):\n",
    "        try:\n",
    "            row['Context_Answer']=' '.join([self.contents_doc[0][1][int(row.Doc_index)-1],row.Answer,self.contents_doc[0][1][int(row.Doc_index)+1]])\n",
    "        except:\n",
    "            print('No context for index:',row.Doc_index)\n",
    "            row['Context Answer']= ' '\n",
    "        return row\n",
    "\n",
    "    def update_count_dic(self,sentences,counter,is_rawtext,remove_index):\n",
    "\n",
    "        for i,c in enumerate(sentences):\n",
    "            counter=counter[is_rawtext].copy()\n",
    "            counter[c]=counter.get(c,0)+1\n",
    "            counter[is_rawtext]=counter\n",
    "            if counter[is_rawtext][c]>1:\n",
    "                remove_index[is_rawtext].append(i)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose your args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_args():\n",
    "\n",
    "    if not arguments['demo']:              \n",
    "        path_q=arguments['query']      \n",
    "        file= open(path_q,\"r+\")  \n",
    "        text=file.read().replace(\"  \",\"\")\n",
    "        queries=text.split(\"\\n\")\n",
    "        queries=[q.split(\"\\t\")[0] for q in queries if len(q)>1]\n",
    "        topics=[q.split(\"\\t\")[1].split(\",\") for q in queries if len(q)>1]\n",
    "        file.close()\n",
    "        \n",
    "    else:\n",
    "        queries=[arguments['demo_query']]\n",
    "        if arguments['model']==5:\n",
    "            topics=arguments['demo_topics']\n",
    "            topics=[topics.split(\",\")]\n",
    "        \n",
    "    pdf_dirs=[arguments['pdf_directory']+arguments['domain']+'/'+arguments['retrieved_company']]\n",
    "    grams=(arguments['min_gram'],arguments['max_gram'])\n",
    "    \n",
    "    args_Infersent={'pdf_directories':pdf_dirs,'w2v_path': arguments['w2v_path'], 'model_path': arguments['model_path'] ,'top_n':arguments['top_n'],'ngram_range':grams,'lemmatize':arguments['lemmatize'],'transform_text':arguments['transform_text'],'l_questions':queries,'sentences_chunk':arguments['sentences_chunk'],'vocab_builder':arguments['vocab_builder']}\n",
    "    args_Bert={'pdf_directories':pdf_dirs,'w2v_path': arguments['w2v_path'], 'model_path': arguments['model_path'] ,'top_n':arguments['top_n'],'ngram_range':grams,'lemmatize':arguments['lemmatize'],'transform_text':arguments['transform_text'],'l_questions':queries,'sentences_chunk':arguments['sentences_chunk'],'vocab_builder':arguments['vocab_builder']}\n",
    "    args_Tf_Idf={'pdf_directories':pdf_dirs,'w2v_path': arguments['w2v_path'], 'model_path': arguments['model_path'] ,'top_n':arguments['top_n'],'ngram_range':grams,'lemmatize':arguments['lemmatize'],'transform_text':arguments['transform_text'],'l_questions':queries,'sentences_chunk':arguments['sentences_chunk'],'vocab_builder':arguments['vocab_builder']}\n",
    "    args_TfBERT={'pdf_directories':pdf_dirs,'w2v_path': arguments['w2v_path'], 'model_path': arguments['model_path'] ,'top_n':arguments['top_n'],'ngram_range':grams,'lemmatize':arguments['lemmatize'],'transform_text':arguments['transform_text'],'l_questions':queries,'sentences_chunk':arguments['sentences_chunk'],'vocab_builder':arguments['vocab_builder']}\n",
    "    args_TfBERT_enhanced={'pdf_directories':pdf_dirs,'w2v_path': arguments['w2v_path'], 'model_path': arguments['model_path'] ,'top_n':arguments['top_n'],'ngram_range':grams,'lemmatize':arguments['lemmatize'],'transform_text':arguments['transform_text'],'l_questions':queries,'sentences_chunk':arguments['sentences_chunk'],'vocab_builder':arguments['vocab_builder'],'topics':topics}\n",
    "    args_All_transforms={'pdf_directories':pdf_dirs,'w2v_path': arguments['w2v_path'], 'model_path': arguments['model_path'] ,'top_n':arguments['top_n'],'ngram_range':grams,'lemmatize':arguments['lemmatize'],'transform_text':arguments['transform_text'],'l_questions':queries,'sentences_chunk':arguments['sentences_chunk'],'vocab_builder':arguments['vocab_builder'],'topics':topics}\n",
    "\n",
    "    if arguments['model']==1:\n",
    "        return args_Infersent\n",
    "    elif arguments['model']==2:\n",
    "        return args_Bert\n",
    "    elif arguments['model']==3:\n",
    "        return args_Tf_Idf\n",
    "    elif arguments['model']==4:\n",
    "        return args_TfBERT\n",
    "    elif arguments['model']==5:\n",
    "        return args_TfBERT_enhanced\n",
    "    else:\n",
    "        print('Select a correct model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*********** READER *************\n",
      "Reading pdfs doc on location: ./data/pdf_files/Tourism/Disney/pdfs/\n",
      "files from ./data/pdf_files/Tourism/Disney/pdfs succesfully converted \n",
      "\n",
      "\n",
      "********* DOCUMENTS RETRIEVED **********\n",
      "FOLDER : 1 , 1721 sentences \n",
      " \n",
      "\n",
      "********* MODEL BERT FINETUNED & TFIDF SHORT TEXT CLUSTERING **********\n",
      "semantic kernel has been retrieved \n",
      "\n",
      "\n",
      "\n",
      " Query : \t Does the company support local agriculture \n",
      " Topics : \t local agriculture \n",
      " Expansion : \t  | organ | product | biodivers \n",
      "\n",
      "********* CLUSTER OF 80  DOCS ********** =  \n",
      "+----+---------------------------------------------------------------------------------------------------------+\n",
      "|    | 0                                                                                                       |\n",
      "|----+---------------------------------------------------------------------------------------------------------|\n",
      "|  0 | It will also feature an array of exclusive original series and movies, along with titles/episodes fr... |\n",
      "|  1 | Always remember that in every interaction, you are the face of our Company.....                         |\n",
      "|  2 | Company • Would be considered excessive under the circumstances • Would violate our Company policies... |\n",
      "|  3 | If you purchase products or services for the Company, put the Company’s interests first and seek to ... |\n",
      "|  4 | Comply with Company policy when it comes to retaining, storing and disposing of Company records.....    |\n",
      "|  5 | You may not trade in Company stock or other securities based on material inside information you have... |\n",
      "|  6 | Working for or having a material financial interest in a company that competes with our Company can ... |\n",
      "|  7 | Certainly when speaking about the Company you should be professional, truthful and accurate.....        |\n",
      "|  8 | Management does not expect the Company to suffer any material liability by reason of these actions..... |\n",
      "|  9 | The foundation of these ongoing programs reflects our values and legacy as a company, as well as the... |\n",
      "| 10 | Breaches of data privacy can expose you and the Company to legal penalties and harm the reputation w... |\n",
      "| 11 | Filed herewith * A signed original of this written statement required by Section 906 has been provid... |\n",
      "| 12 | The Company also sells merchandise to retailers under wholesale arrangements.....                       |\n",
      "| 13 | If you comment online regarding any aspect of Company business, identify yourself as an employee and... |\n",
      "| 14 | Management does not believe that the Company has incurred a probable material loss by reason of any ... |\n",
      "| 15 | Our Company does not tolerate any form of retaliation against anyone who makes a good faith report o... |\n",
      "| 16 | As a Cast Member or employee, your job may expose you to material, nonpublic (or “inside”) informati... |\n",
      "| 17 | In addition, collecting information by misrepresenting facts, employee identity or Company affiliati... |\n",
      "| 18 | For convenience, the terms “Company” and “we” are used to refer collectively to the parent company a... |\n",
      "| 19 | The Company creates, distributes, licenses and publishes a variety of products in multiple countries... |\n",
      "| 20 | Proprietary information – Information that a company owns that represents the work it does.....         |\n",
      "| 21 | With respect to intellectual property developed by the Company and rights acquired by the Company fr... |\n",
      "| 22 | The Company also licenses Disney properties and content to mobile phone carriers in Japan.....          |\n",
      "| 23 | The Company does not enter into these transactions or any other hedging transactions for speculative... |\n",
      "| 24 | Unless you are an authorized representative, you should not speak on behalf of the Company.....         |\n",
      "| 25 | The Company’s dayparts are: primetime, daytime, late night, news and sports (includes broadcast and ... |\n",
      "| 26 | The Company’s vacation club units range from deluxe studios to three-bedroom grand villas.....          |\n",
      "| 27 | The Company distributes these products through its own distribution and marketing companies in the U... |\n",
      "| 28 | Our Company may be liable if a bribe is paid on our behalf, even if we did not authorize it to be pa... |\n",
      "| 29 | Bribery Act – A law that makes it illegal for anyone (public, quasi-public or private) working for a... |\n",
      "| 30 | The Company licenses our television and film content to Hulu in the ordinary course of business.....    |\n",
      "| 31 | The Company’s Media Networks businesses also compete with other media and entertainment companies, S... |\n",
      "| 32 | The VAR model is a risk analysis tool and does not purport to represent actual losses in fair value ... |\n",
      "| 33 | Company does not tolerate any form of retaliation (including separation, demotion, suspension or los... |\n",
      "| 34 | The Company defers a portion of its profits from transactions with investees.....                       |\n",
      "| 35 | It represents one of our Company’s most valuable assets and should never be used for your personal b... |\n",
      "| 36 | From time to time, you may be offered gifts from a person or a company that does – or seeks to do bu... |\n",
      "| 37 | The amounts that the Company has recorded are provisional estimates of the impact the Tax Act will h... |\n",
      "| 38 | The consolidated financial statements of the Company include the accounts of The Walt Disney Company... |\n",
      "| 39 | If you or a member of your immediate family has a material financial interest in a company that is o... |\n",
      "| 40 | The Company’s internet websites and digital products compete with other websites and entertainment p... |\n",
      "| 41 | This year, the company provided a grant to My Brother’s Keeper Alliance to support the organization’... |\n",
      "| 42 | Note that the Company’s Senior Vice President of Government Relations must approve any corporate con... |\n",
      "| 43 | The Walt Disney Company 2828 Also, in certain circumstances, using market power to coerce buyers to ... |\n",
      "| 44 | These requirements apply to the Company’s own sourcing activities as well as to licensees, vendors, ... |\n",
      "| 45 | We are a diversified entertainment company that offers entertainment, travel and consumer products w... |\n",
      "| 46 | Walt Disney Imagineering Walt Disney Imagineering provides master planning, real estate development,... |\n",
      "| 47 | The Company’s operations include retail, wholesale and online distribution of products.....             |\n",
      "| 48 | We continue to build the capacity of our consumer product licensees and vendors, partner with extern... |\n",
      "| 49 | We distribute the Company’s productions worldwide to television broadcasters, SVOD services (includi... |\n",
      "| 50 | Disney is a longtime supporter of scholarship organizations that make it possible for highachieving ... |\n",
      "| 51 | The Company licenses characters from its film, television and other properties for use on third-part... |\n",
      "| 52 | • The domestic production activity deduction was eliminated effective for the Company’s fiscal 2019.... |\n",
      "| 53 | The Company’s merchandise licensing operations cover a diverse range of product categories, the most... |\n",
      "| 54 | The acquisition supports the Company’s strategy to launch DTC video streaming services.....             |\n",
      "| 55 | The Company capitalizes interest on assets constructed for its parks and resorts and on certain film... |\n",
      "| 56 | In addition to our company VoluntEARS program, employees around the world supported community nonpro... |\n",
      "| 57 | The products include children’s books, comic books, graphic novel collections, magazines, learning p... |\n",
      "| 58 | The Consumer Products & Interactive Media segment licenses the Company’s trade names, characters and... |\n",
      "| 59 | The safety of products bearing Disney brands, characters and other intellectual property is of cruci... |\n",
      "| 60 | Don’t misrepresent the characteristics or capabilities of our products or recommend products or serv... |\n",
      "| 61 | New technologies affect the demand for our products, the manner in which our products are distribute... |\n",
      "| 62 | The Company’s trade receivables and financial investments do not represent a significant concentrati... |\n",
      "| 63 | Changes in technology and in consumer consumption patterns may affect demand for our entertainment p... |\n",
      "| 64 | The Company also produces stage plays and musical recordings, licenses and produces live entertainme... |\n",
      "| 65 | Do your part to meet our high standards, whether you are designing, building, operating or maintaini... |\n",
      "| 66 | Through these contributions, we supported a broad range of community programs and organizations that... |\n",
      "| 67 | • Certain provisions of the Act are not effective for the Company until fiscal 2019 including: • The... |\n",
      "| 68 | A significant number of companies produce and/or distribute theatrical and television films, exploit... |\n",
      "| 69 | Home entertainment sales vary based on the number and quality of competing home entertainment produc... |\n",
      "| 70 | Film and television production costs include all internally produced content such as live-action and... |\n",
      "| 71 | Regardless of local practice or the practices of other companies, make sure you avoid even the appea... |\n",
      "| 72 | At the Alto Mayo carbon project in Peru supported by Disney, conservation efforts enabled farmers to... |\n",
      "| 73 | Under the Satellite Home Viewer Improvement Act and its successors, including most recently the STEL... |\n",
      "| 74 | We are dedicated to delivering quality products and services and cooperating with community leaders ... |\n",
      "| 75 | The Company has bank facilities with a syndicate of lenders to support commercial paper borrowings a... |\n",
      "| 76 | • Never represent that a product has been inspected or labeled as fit for use if it hasn’t been.....    |\n",
      "| 77 | Specifically, the following issues, among others, must be addressed in combining the operations of 2... |\n",
      "| 78 | Programming costs include film or television product licensed for a specific period from third parti... |\n",
      "| 79 | A variety of uncontrollable events may reduce demand for our products and services, impair our abili... |\n",
      "+----+---------------------------------------------------------------------------------------------------------+\n",
      "Converting eval set to tfrecord...\n",
      "WARNING:tensorflow:From /Users/williamlambert/Desktop/heavy_harshQA/harshQA_reranker/harshQA_tfrecord.py:60: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x1a44ac16a8>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './data/output/bert_checkpoints', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a44ac3a58>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=10, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
      "INFO:tensorflow:Could not find trained model in model_dir: ./data/output/bert_checkpoints, running initialization to predict.\n",
      "WARNING:tensorflow:From /Users/williamlambert/Desktop/heavy_harshQA/harshQA_reranker/harshQA_bert_builder.py:134: The name tf.FixedLenSequenceFeature is deprecated. Please use tf.io.FixedLenSequenceFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/williamlambert/Desktop/heavy_harshQA/harshQA_reranker/harshQA_bert_builder.py:138: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/williamlambert/Desktop/heavy_harshQA/harshQA_reranker/harshQA_bert_builder.py:140: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running infer on CPU\n",
      "WARNING:tensorflow:From /Users/williamlambert/Desktop/heavy_harshQA/harshQA_reranker/modeling.py:172: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/williamlambert/Desktop/heavy_harshQA/harshQA_reranker/modeling.py:411: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/williamlambert/Desktop/heavy_harshQA/harshQA_reranker/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /Users/williamlambert/Desktop/heavy_harshQA/harshQA_reranker/harshQA_bert_builder.py:91: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /Users/williamlambert/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "*******************************  RESULTS of BERT  ***************************************\n",
      "+-----------+--------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|           |      0 | 1                                                                                                                        |\n",
      "|-----------+--------+--------------------------------------------------------------------------------------------------------------------------|\n",
      "| Answer °0 | 0.0013 | Certainly when speaking about the Company you should be professional, truthful and accurate.                             |\n",
      "| Answer °1 | 0.0012 | The Company has bank facilities with a syndicate of lenders to support commercial paper borrowings as follows:Committed  |\n",
      "| Answer °1 | 0.0012 | Capacity Capacity Used.                                                                                                  |\n",
      "| Answer °2 | 0.001  | At the Alto Mayo carbon project in Peru supported by Disney, conservation efforts enabled farmers to participate in an o |\n",
      "| Answer °2 | 0.001  | rganic fair trade coffee co-op.                                                                                          |\n",
      "| Answer °3 | 0.0009 | The Company’s operations include retail, wholesale and online distribution of products.                                  |\n",
      "| Answer °4 | 0.0008 | Regardless of local practice or the practices of other companies, make sure you avoid even the appearance of doing somet |\n",
      "| Answer °4 | 0.0008 | hing improper.                                                                                                           |\n",
      "+-----------+--------+--------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module '__main__' has no attribute 'main'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4f472e83bfc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtabulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer °'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexs_tiled\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'keys'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtablefmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'psql'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;34m\"\"\"Runs the program with an optional 'main' function and 'argv' list.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module '__main__' has no attribute 'main'"
     ]
    }
   ],
   "source": [
    "window_size=80\n",
    "dic_suffix={1:'FASTEXT',2:'BERT',3:'TFIDF',4:'BERTCLUST',5:'BERTCLUST_TUNED'}\n",
    "args=collect_args()\n",
    "check_args()\n",
    "args_fit={key:value for key,value in args.items() if key not in ['l_questions','topics']}\n",
    "QAmodel=QApipeline(**args) \n",
    "QAmodel.fit_reader()\n",
    "QAmodel.fit()\n",
    "results=QAmodel.predict(args['l_questions'],args.get('topics',[]))\n",
    "\n",
    "if not arguments['demo']:\n",
    "    dir=arguments['output_dir']+\"/\"+arguments['domain']+\"/\"+arguments['retrieved_company']\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    results.to_csv(dir+'_result.csv')\n",
    "else:\n",
    "    print('*******************************  RESULTS of BERT  ***************************************')\n",
    "    show=pd.DataFrame(results[['Score','Answer']].values,index=range(0,100*(arguments['top_n']),100),columns=['Score','Answer'])\n",
    "    counter_newline={}\n",
    "    for i,aw in enumerate(show.Answer.tolist()):\n",
    "        dividend,quotient=len(aw)//window_size,len(aw)%window_size\n",
    "        if quotient!=0:\n",
    "            dividend+=1\n",
    "        counter_newline[i]=dividend\n",
    "\n",
    "\n",
    "        for newline in range(dividend):\n",
    "            show.loc[(100*i)+(newline)]=[show.loc[100*i]['Score'],aw[window_size*newline:window_size*(newline+1)]]\n",
    "    show=show.reset_index().sort_values(by='index')[['Score','Answer']].values\n",
    "    indexs_tiled=np.concatenate([np.tile([i],counter_newline[i]) for i in range(arguments['top_n'])])\n",
    "    print(tabulate(pd.DataFrame(show,index=['Answer °'+ str(i) for i in indexs_tiled]), headers='keys', tablefmt='psql'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1721]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QAmodel.borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Question Answering -  ESG Assessment Projects BNP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">PARTIE CODE -------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code dependencies of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Pipeline for Question Answering on closed domain and non factoid questions - harshQA\n",
    "## Developped by William Lambert  (Risk AIR Team , BNP Paribas)\n",
    "\n",
    "import warnings\n",
    "from utils.utils  import hide_warn\n",
    "warnings.warn=hide_warn\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import uuid\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from string import digits\n",
    "from sklearn.base import BaseEstimator\n",
    "from tabulate import tabulate\n",
    "\n",
    "#Bert_dependencies\n",
    "from harshQA_reranker.tokenization import* \n",
    "import harshQA_reranker.metrics as metrics\n",
    "import harshQA_reranker.modeling as modeling\n",
    "import harshQA_reranker.optimization as optimization \n",
    "\n",
    "#Import our pdf reader\n",
    "from harshQA_pdf_reader.reader import pdfconverter\n",
    "\n",
    "#Import bert finetuned pipeline\n",
    "from harshQA_reranker.harshQA_tfrecord import *\n",
    "from harshQA_reranker.harshQA_bert_builder import * \n",
    "from harshQA_reranker.harshQA_run_msmarco import run_msmarco\n",
    "\n",
    "#Import all our models, wrap in a scikit learn estimator\n",
    "from harshQA_retrievers.m1__Infersent import m1_Infersent\n",
    "from harshQA_retrievers.m2__Bert import m2_Bert\n",
    "from harshQA_retrievers.m3__Tfidf import m3_Tfidf\n",
    "from harshQA_retrievers.m5__harshQA import m5_harshQA\n",
    "\n",
    "#Utils\n",
    "from utils.utils import remove_non_alpha\n",
    "from utils.utils import generate_querries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'model' is defined twice. First from /Users/williamlambert/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py, Second from /Users/williamlambert/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py.  Description from first occurrence: ** Select a model \n **Model without pre-clustering: \n\t 1-Infersent_glove [Pretrained] (10 min/corpus) \n\t 2-Bert [Pretrained on SQUAD] (30 min/corpus) \n \t 3-Tf_Idf_Lemmatizer [Trained on our corpus] (5 min/ corpus) \n**Model with pre-clustering\\t 4-Tf-Idf_Bert [Pretrained on SQuAD] (3 min/query)\n\t 5-Tf-Idf_Bert_enhanced [Finetuned on MsMarco] (1:30 min/query)\n\t 6- All \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a8fb5940e4b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m flags.DEFINE_integer(\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \"** Select a model \\n **Model without pre-clustering: \\n\\t 1-Infersent_glove [Pretrained] (10 min/corpus) \\n\\t 2-Bert [Pretrained on SQUAD] (30 min/corpus) \\n \\t 3-Tf_Idf_Lemmatizer [Trained on our corpus] (5 min/ corpus) \\n**Model with pre-clustering\\\\t 4-Tf-Idf_Bert [Pretrained on SQuAD] (3 min/query)\\n\\t 5-Tf-Idf_Bert_enhanced [Finetuned on MsMarco] (1:30 min/query)\\n\\t 6- All \\n\")\n\u001b[0m\u001b[1;32m      7\u001b[0m \"\"\"\n\u001b[1;32m      8\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_integer\u001b[0;34m(name, default, help, lower_bound, upper_bound, flag_values, **args)\u001b[0m\n\u001b[1;32m    313\u001b[0m   \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegerParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m   \u001b[0mDEFINE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m   \u001b[0m_register_bounds_validator_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflag_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, **args)\u001b[0m\n\u001b[1;32m     80\u001b[0m   \"\"\"\n\u001b[1;32m     81\u001b[0m   DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\n\u001b[0;32m---> 82\u001b[0;31m               flag_values, module_name)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'model' is defined twice. First from /Users/williamlambert/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py, Second from /Users/williamlambert/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py.  Description from first occurrence: ** Select a model \n **Model without pre-clustering: \n\t 1-Infersent_glove [Pretrained] (10 min/corpus) \n\t 2-Bert [Pretrained on SQUAD] (30 min/corpus) \n \t 3-Tf_Idf_Lemmatizer [Trained on our corpus] (5 min/ corpus) \n**Model with pre-clustering\\t 4-Tf-Idf_Bert [Pretrained on SQuAD] (3 min/query)\n\t 5-Tf-Idf_Bert_enhanced [Finetuned on MsMarco] (1:30 min/query)\n\t 6- All \n"
     ]
    }
   ],
   "source": [
    "flags = tf.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"model\",5,\n",
    "    \"** Select a model \\n **Model without pre-clustering: \\n\\t 1-Infersent_glove [Pretrained] (10 min/corpus) \\n\\t 2-Bert [Pretrained on SQUAD] (30 min/corpus) \\n \\t 3-Tf_Idf_Lemmatizer [Trained on our corpus] (5 min/ corpus) \\n**Model with pre-clustering\\\\t 4-Tf-Idf_Bert [Pretrained on SQuAD] (3 min/query)\\n\\t 5-Tf-Idf_Bert_enhanced [Finetuned on MsMarco] (1:30 min/query)\\n\\t 6- All \\n\")\n",
    "\"\"\"\n",
    "*** \n",
    "** Model without pre-clustering:\n",
    "        1-Infersent_glove [Pretrained] (10 min/corpus)\n",
    "        2-Bert [Pretrained on SQUAD] (30 min/corpus)\n",
    "        3-Tf_Idf_Lemmatizer [Trained on our corpus] (5 min/ corpus)\n",
    "        \n",
    "* *Model with pre-clustering\n",
    "        4-Tf-Idf_Bert [Pretrained on SQuAD] (3 min/query)\n",
    "        5-Short text clustering (enhanced tf-idf) and Bert retriever finetuned on MsMarco (1:30 min/query)\n",
    "\n",
    "\n",
    "** The settings of our test was: *** \n",
    "  Run on CPU\n",
    "  size_cluster=50\n",
    "  Corpus of text was 1500 sentences(300 pages) and 15 queries\n",
    "  Corpus of domain_vocab was 3000 sentences (600 pages) \n",
    "  The timespeed of pdf converter is approximately 10s/1000 pages\\\n",
    "  The best results were achieved with model5 harshQA ; see our notebook : harshQA_eval\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "flags.DEFINE_boolean(\n",
    "    \"demo\",True,\n",
    "    \"Demo mode with your own pdfs.\")\n",
    "\n",
    "flags.DEFINE_string(\"demo_query\",\"Does the company reduce its ghg emissions?\",\n",
    "                    \"Query if demo activated.\")\n",
    "\n",
    "flags.DEFINE_string(\"demo_topics\",\"reduce emissions, ghg emissions\",\n",
    "                    \"Topics (coma separated), if demo activated.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"top_n\",5,\n",
    "    \"Number of doc to retrieve per query\")\n",
    "\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"query_dir\",'./utils/pdf_files/Tourism/Queries.txt',\n",
    "    \"Path of the .txt file where your queries are located\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"size_cluster\",100,\n",
    "    \"size of the clusters of candidate to feed in neural network.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"domain\", \"Tourism\" ,\n",
    "    \"Domain folder name to process Q&A.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"retrieved_company\",\"Disney\",\n",
    "    \"Company folder name to query.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"pdf_directory\",\n",
    "    './utils/pdf_files/',\n",
    "    \"Path of the pdf directory.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"vocab_file\",\n",
    "    \"./data/bert/pretrained_models/uncased_L-12_H-768_A-12/vocab.txt\",\n",
    "    \"The vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "     \"whole_corpus\",\n",
    "    \"./utils/pdf_files/All\",\n",
    "    \"The whole corpus to fit before predicting, smaller corpus, i.e the corpus\"\n",
    "    \"to build and save the tfidfs weights, vocab and semantic kernel.\" )\n",
    "\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"vocab_builder\",\n",
    "    \"./corpusESG.json\",\n",
    "    \"The path to build our own vocabulary file for the pdf-reader module: will speed up reading ! \")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"w2v_path\",\n",
    "    \"./data/fastText/crawl-300d-2M.vec\",\n",
    "    \"Path of the .vec file of GloVe or FastText.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"model_path\",\n",
    "    \"./data/encoder/infersent2.pkl\",\n",
    "    \"Path of the .pkl file of infersent model.\")    \n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\",\n",
    "    \"./output\",\n",
    "    \"The output directory where all the model bert,tfidf checkpoints will be written after train \"\n",
    "    \"Will also store the raw tsv and tfrecords predictions.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"bert_config_file\", \"./data/bert_msmarco/bert_config.json\",\n",
    "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\",\n",
    "    \"./data/bert_msmarco/model.ckpt\",\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 512,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter\"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_query_length\", 128,\n",
    "    \"The maximum query sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated.\")\n",
    "    \n",
    "flags.DEFINE_boolean(\n",
    "    \"msmarco_output\", True,\n",
    "    \"Whether to write the predictions to a MS-MARCO-formatted file.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_eval\", True, \"Whether to predict or not.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 50, \"Total batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"eval_batch_size\", 50, \"Total batch size for eval.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 1e-6, \"The initial learning rate for Adam.\")\n",
    "\n",
    "flags.DEFINE_integer(\"num_train_steps\", 400000,\n",
    "                     \"Total number of training steps to perform.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_eval_examples\", None,\n",
    "                     \"Maximum number of examples to be evaluated.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"num_warmup_steps\", 40000,\n",
    "    \"Number of training steps to perform linear learning rate warmup.\")\n",
    "\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 100,\n",
    "                     \"How often to save the model checkpoint.\")\n",
    "\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 10,\n",
    "                     \"How many steps to make in each estimator call.\")\n",
    "\n",
    "flags.DEFINE_integer(\"min_gram\",1,\n",
    "                    \"min grams to use for tf-idf model retriever.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_gram\",1,\n",
    "                    \"max grams to use for tf-idf model retriever.\")\n",
    "\n",
    "flags.DEFINE_bool(\"lemmatize\",False,\n",
    "                    \"wether to use lemmas instead of stems (not advised at all).\")\n",
    "\n",
    "flags.DEFINE_bool(\"transform_text\",True,\n",
    "                    \"wether to use transform text or not (stemmning by default).\")\n",
    "\n",
    "\n",
    "flags.DEFINE_integer(\"sentences_chunk\",1,\n",
    "                    \"wether to use block of one or two sentences (min=1 , max=2) as answers\"\n",
    "                     \"Keep in mind that we return the context of each sentences at the end of the prediction loop,\"\n",
    "                    \"i.e the previous and next sentence of the answer.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_name\", None,\n",
    "    \"The Cloud TPU to use for training. This should be either the name \"\n",
    "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
    "    \"url.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_zone\", None,\n",
    "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"num_tpu_cores\", 8,\n",
    "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_args():\n",
    "    \n",
    "    assert tf.__version__[0]=='1' , \"This code has been implemented on tf 1.14, if you want to use you should modify the code\"\n",
    "    assert FLAGS.retrieved_company!=None, \"Select a Company\"\n",
    "    assert FLAGS.domain!=None, \"Select a Domain\"\n",
    "    assert FLAGS.model!=None, \"Select a Model\"\n",
    "    assert FLAGS.pdf_directory!=None, \"Select a $PATH which contains the folder Domain/Company/pdfs/ containing pdf files to query\"\n",
    "    assert FLAGS.whole_corpus!=None, \"Select a pdf directory path which contains all the pdf of the corpus to fit our model\"\n",
    "    assert FLAGS.vocab_builder!=None, \"Enter a .json path to save all our vocabulary while ingesting pdf files \"\n",
    "    assert FLAGS.lemmatize==False or FLAGS.model==3 ,\"Lemmatize option is only available for Tf-Idf model (model n°3)\"\n",
    "    assert FLAGS.max_gram==1 or FLAGS.model==3 , \"Multi gram option is only available for Tf-Idf model (model n°3)\"\n",
    "    assert FLAGS.transform_text==True, \"Your model will work better with a tokenizer that used (stemming/lemmatizing)\"\n",
    "    \n",
    "    if FLAGS.model ==5:\n",
    "        assert FLAGS.eval_batch_size<=FLAGS.size_cluster,\"eval batch size should be less than the size of the cluster of preselected sentences\"\n",
    "        assert FLAGS.size_cluster%FLAGS.eval_batch_size==0,\"eval batch size should be a multiple of the size of the cluster of preselected sentences\"\n",
    "        assert FLAGS.bert_config_file!=None, \"Enter a .json bert config file to specify the model architecture\"\n",
    "        assert FLAGS.vocab_file!=None, \"Enter the path of uncased_L-12_H-768_A-12/vocab.txt\"\n",
    "        assert \"uncased_L-12_H-768_A-12\" in FLAGS.vocab_file, \"You need to pass the vocab file of bert uncased L-12_H-768_A-12 \"\n",
    "        assert \".txt\" in FLAGS.vocab_file, \"The bert vocab_file must be a .txt file\"\n",
    "        assert \".json\" in FLAGS.bert_config_file, \"The bert_config_file must be a .json file\"\n",
    "        assert FLAGS.output_dir!=None, \"Enter the output directory where all the model bert,tfidf checkpoints will be written after train \"\n",
    "        \"It will also store the raw tsv files and the tfrecords used to feed bert-reranker.\"\n",
    "        assert FLAGS.init_checkpoint!=None,\"Enter a bert .ckpt init checkpoint\"\n",
    "        assert \".ckpt\" in FLAGS.init_checkpoint, \"The init_checkpoint must be a .ckpt file\"\n",
    "        \n",
    "    \n",
    "        \n",
    "    if FLAGS.model in [1,5]:\n",
    "        assert FLAGS.w2v_path!=None,\"Specify the .vec file path of GloVe or fasText\"\n",
    "        assert \".vec\" in FLAGS.w2v_path, \"The w2v path of GloVe or fasText muste be a .vec file\"\n",
    "        assert FLAGS.model_path!=None,\"Specify the .pkl file path of Infersent2\"\n",
    "        assert \".pkl\" in FLAGS.model_path, \"The infersent model file muste be a .pkl file\"\n",
    "            \n",
    "    \n",
    "    if FLAGS.demo:\n",
    "        assert FLAGS.demo_query!=None, \"Specify a query for the demo\"\n",
    "        if FLAGS.model==5:\n",
    "            assert FLAGS.demo_topics!=None, \"Specify coma separated topics linked to your query for the demo\"\n",
    "    else:\n",
    "        assert FLAGS.query_path!=None , 'Specify a .txt file containing your queries line by line'\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QApipeline():\n",
    "    \"\"\"\n",
    "    ## kwargs :\n",
    "    path_to_directory (str) : Directory to train language model tfidf\n",
    "    ngram_range=(1,3) (tuple) : ngram range for tfidf words\n",
    "    max_df=0.85 (float) : When building the vocabulary ignore terms that have a document frequency strictly \n",
    "                        ## higher than the given threshold (corpus-specific stop words). \n",
    "                        ## If float, the parameter represents a proportion of documents, integer absolute counts. \n",
    "                        ## This parameter is ignored if vocabulary is not None\n",
    "    stop_words (str) : Language for tfidf (default 'english')\n",
    "    paragraphs=None\n",
    "    top=3 (int) : How many paragraphs to retrieve for querry matching (can be modified by threshold)\n",
    "    verbose=False : print time of execution to build tfidf matrix and print errors\n",
    "    MODEL_PATH = infersent_model \n",
    "    W2V_PATH = infersent_path  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,**kwargs):\n",
    "        \n",
    "        \n",
    "        #new kwargs: 'threshold' (float between 0.5 and 1.0)\n",
    "        \n",
    "        self.kwargs_converter = {key: value for key, value in kwargs.items()\n",
    "                            if key in pdfconverter.__init__.__code__.co_varnames}\n",
    "        \n",
    "        self.kwargs_Tf_Idf = {key: value for key, value in kwargs.items()\n",
    "                         if key in m3_Tfidf.__init__.__code__.co_varnames}\n",
    "        \n",
    "        self.kwargs_Infersent={key: value for key, value in kwargs.items()\n",
    "                         if key in m1_Infersent.__init__.__code__.co_varnames}\n",
    "        self.kwargs_Bert={key: value for key, value in kwargs.items()\n",
    "                         if key in m2_Bert.__init__.__code__.co_varnames}\n",
    "        \n",
    "        self.usemodel=FLAGS.model\n",
    "        self.sentences_chunk=kwargs['sentences_chunk']\n",
    "        \n",
    "        #CALL PDF READER \n",
    "        print('{}*********** READER *************'.format('\\n'))\n",
    "        print(\"Reading pdfs doc on location: {}\".format(FLAGS.pdf_directory+FLAGS.domain+'/'+FLAGS.retrieved_company+'/pdfs/'))\n",
    "        self.df=pdfconverter(**self.kwargs_converter).transform()\n",
    "                \n",
    "        #BUILD CONTENT AND DOCUMENT INDEX \n",
    "        self.content=[]\n",
    "        self.content_raw=[]\n",
    "        self.contents_doc=[]\n",
    "        self.borders=[0]\n",
    "\n",
    "        print('********* DOCUMENTS RETRIEVED **********')\n",
    "        for j,repo in enumerate(sorted(list(set(self.df.directory_index)))):\n",
    "            \n",
    "            count_dic=[{},{}]\n",
    "            remove_idx=[[],[]]\n",
    "            content_doc=[]\n",
    "            content_doc_raw=[]\n",
    "            \n",
    "            title=self.df[self.df.directory_index==repo].directory.tolist()[0]\n",
    "            self.df[self.df.directory_index==repo]['raw paragraphs'].apply(lambda sentences: self.update_count_dic(sentences,count_dic,0,remove_idx))\n",
    "            self.df[self.df.directory_index==repo]['paragraphs'].apply(lambda sentences: self.update_count_dic(sentences,count_dic,1,remove_idx))\n",
    "            self.df[self.df.directory_index==repo]['raw paragraphs'].apply(lambda sentences: content_doc_raw.extend(sentences))\n",
    "            self.df[self.df.directory_index==repo]['paragraphs'].apply(lambda sentences: content_doc.extend(sentences))\n",
    "            \n",
    "            #REMOVE TWIN SENTENCES AND REMOVE TOO SMALL SENTENCES\n",
    "            remove_idx=list(set(remove_idx[0]+remove_idx[1]))\n",
    "            content_doc=np.delete(np.array(content_doc),remove_idx)\n",
    "            content_doc_raw=np.delete(np.array(content_doc_raw),remove_idx)\n",
    "            \n",
    "            content=[content_doc[i] for i in range(len(content_doc)) if (len(content_doc[i])>=50 )]\n",
    "            content_raw=[content_doc_raw[i] for i in range(len(content_doc)) if (len(content_doc[i])>=50)]\n",
    "            self.borders.append(len(content))\n",
    "            \n",
    "            try:\n",
    "                print(\"FOLDER : {} , {} sentences \\n \\n\".format(self.df.directory.unique()[j],len(content)))\n",
    "            except:\n",
    "                print(\"FOLDER : {} , {} sentences \\n \\n\".format(\"Similar reports\",len(content)))\n",
    "            \n",
    "            #ADD SENTENCES TO OUR FINAL OBJECTS\n",
    "            self.content.extend(list(content))\n",
    "            self.content_raw.extend(list(content_raw))\n",
    "            self.contents_doc.append([content,content_raw])\n",
    "        \n",
    "        \n",
    "        self.borders=list(np.cumsum(self.borders))\n",
    "        \n",
    "        #GROUP SENTENCES BY PAIR EVENTUALLY\n",
    "        if self.sentences_chunk==2:\n",
    "\n",
    "            self.content=[ ' '.join(x) for x in zip(self.content[0::2], self.content[1::2]) ]\n",
    "            self.content_raw=[ ' '.join(x) for x in zip(self.content_raw[0::2], self.content_raw[1::2]) ]\n",
    "            for i,(treated_sentences,raw_sentences) in enumerate(self.contents_doc):\n",
    "                self.contents_doc[i][0]=[ ' '.join(x) for x in zip(treated_sentences[0::2], treated_sentences[1::2]) ]\n",
    "                self.contents_doc[i][1]=[ ' '.join(x) for x in zip(raw_sentences[0::2], raw_sentences[1::2]) ]\n",
    "            self.borders=[int(i/2) for i in self.borders]\n",
    "\n",
    "        #REPLACE ALL DIGITS WITH SPECIAL TOKEN FOR OUR MODEL\n",
    "        for i,c in enumerate(self.borders[:-1]):\n",
    "            start_idx=self.borders[i]\n",
    "            content=self.contents_doc[i][0]\n",
    "            content_raw=self.contents_doc[i][1]\n",
    "\n",
    "            #ADD TREATED TEXT TO CONTENTS_DOC\n",
    "            for sentence_id,sentence in enumerate(content):\n",
    "                \n",
    "                words_list=sentence.split(\" \")\n",
    "                for word_id,w in enumerate(words_list):\n",
    "                    try:\n",
    "                        float(w)\n",
    "                        words_list[word_id]=\"XXX\"\n",
    "                    except:\n",
    "                        words_list[word_id]=w\n",
    "                    self.content[start_idx+sentence_id]=\" \".join(words_list)\n",
    "                    \n",
    "                self.contents_doc[i][0][sentence_id]=\" \".join(words_list)\n",
    "    \n",
    "    def fit(self):\n",
    "        \n",
    "        MODELS=['INFERSENT - GLOVE','BERT PRETRAINED','TFIDF - LEMMATIZER & BIGRAM','BERT & TFIDF SHORT TEXT CLUSTERING','BERT FINETUNED & TFIDF SHORT TEXT CLUSTERING']\n",
    "        print('********* MODEL {} **********'.format(MODELS[self.usemodel-1]))\n",
    "        \n",
    "        if self.usemodel==1:\n",
    "            #Fit Infersent\n",
    "            self.model_retriever = m1_Infersent(**self.kwargs_Infersent)\n",
    "            self.model_retriever.fit(self.content)\n",
    "            self.model_retriever.transform(self.content)\n",
    "        \n",
    "        if self.usemodel==2:\n",
    "            #Fit Bert pretrained\n",
    "      \n",
    "            self.model_retriever=m2_Bert(**self.kwargs_Bert)\n",
    "            self.model_retriever.fit(self.content)#no finetuning for Bert\n",
    "            self.model_retriever.transform(self.content)\n",
    "           \n",
    "        if self.usemodel==3:\n",
    "            #Fit Tf-Idf for mixture models or single tf-idf Lemmatize Bigram\n",
    " \n",
    "            self.kwargs_Tf_Idf['save_idfs_path']=None\n",
    "            self.kwargs_Tf_Idf['save_features_path']=None\n",
    "            self.model_retriever = m3_Tfidf(**self.kwargs_Tf_Idf)\n",
    "            self.model_retriever.fit(self.content)\n",
    "            self.model_retriever.transform(self.content)\n",
    " \n",
    "        \n",
    "        #harshQA model \n",
    "        if self.usemodel==5:\n",
    "           \n",
    "            output_TF=FLAGS.output_dir+'/tf_idf_checkpoints/'\n",
    "            \n",
    "            args_harshQA={'save_kernel_path':output_TF+'kernel.npy',\n",
    "                          'save_kernel_vocab_path':output_TF+'kernel_vocab.json',\n",
    "                          'save_kernel_idx_path':output_TF+'kernel_vocab_idx.json',\n",
    "                          'save_idfs_path':output_TF+'idfs.npy',\n",
    "                          'save_features_path':output_TF+'vocab.json'}\n",
    "            \n",
    "            for key, value in FLAGS.flag_values_dict().items():\n",
    "                if key in m5_harshQA.__init__.__code__.co_varnames:\n",
    "                    args_harshQA[key]=value\n",
    "                    \n",
    "            self.model_retriever=m5_harshQA(**args_harshQA)\n",
    "            self.model_retriever.fit(self.content)\n",
    "            self.model_retriever.transform(self.contents_doc[0])\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    \n",
    "        #Initialisation of Tf-Idf-Farahat\n",
    "    \n",
    "    def predict(self,Qst):\n",
    "        \"\"\"\n",
    "        kwargs:\n",
    "        ##VE_type: 'DP' for Detect Presence of 'VE' for Value extraction\n",
    "        ##Qst: Querry\n",
    "        ##VE_cdt : null\n",
    "        \"\"\"\n",
    "        self.Qst_raw=Qst\n",
    "        repo_to_query=0\n",
    "\n",
    "        #Apply corpus transformations to querry for parcimony\n",
    "        newQst=[q.lower() for q in self.Qst_raw]\n",
    "        newQst=[remove_non_alpha(q) for q in newQst]\n",
    "        newQst=[q.replace('.','') for q in newQst]\n",
    "\n",
    "        self.dataframe=[]        \n",
    "        all_scores=[]\n",
    "        all_models=[]\n",
    "        all_querries=[]\n",
    "        all_ranks=[]\n",
    "        all_indices=[]\n",
    "        all_answers=[]\n",
    "\n",
    "        #Infersent retriever\n",
    "        if self.usemodel !=5:\n",
    "            for i,qu in enumerate(newQst):\n",
    "\n",
    "                indices,scores=self.model_retriever.predict(qu,[self.borders[repo_to_query],self.borders[repo_to_query+1]])\n",
    "                p=len(indices)\n",
    "                all_scores.extend(scores.loc[indices].values[:,0])\n",
    "                all_answers.extend([ self.contents_doc[repo_to_query][1][i] for i in indices])\n",
    "                all_models.extend(MODELS[FLAGS.model-1]*p)\n",
    "                all_ranks.extend(list(range(1,p+1)))\n",
    "                all_querries.extend([self.Qst_raw[i]]*p)\n",
    "                all_indices.extend(indices)\n",
    "            self.dataframe=pd.DataFrame(np.c_[all_querries,all_models,all_ranks,all_indices,all_answers,all_scores],columns=['Question','Model','Rank','Doc_index','Answer','Score'])\n",
    "                 \n",
    "        \n",
    "        #harshQA retriever\n",
    "        else:\n",
    "            self.dataframe=self.model_retriever.predict(self.Qst_raw,self.topics)\n",
    "      \n",
    "\n",
    "        #FORMAT THE OUTPUT NICELY AND RETURN IT\n",
    "        self.dataframe=self.dataframe.apply(self.add_ctxt,axis=1)\n",
    "        self.dataframe['Rank']=self.dataframe['Rank'].map(lambda x: x[0])\n",
    "        self.dataframe['Score']=self.dataframe['Score'].map(lambda x: np.round(float(x),4))\n",
    "        self.dataframe['Company']=[FLAGS.retrieved_company]*len(self.dataframe)\n",
    "        self.dataframe=self.dataframe.sort_values(by=['Question','Company','Model','Rank']).reset_index(drop=True)[['Question','Company','Model','Answer','Rank','Score','Doc_index','Context_Answer']]\n",
    "        return self.dataframe\n",
    "            \n",
    "\n",
    "    def string_retriever(self,sentence_list):\n",
    "        return [w  for w in sentence_list if not w.isdigit()]\n",
    "    \n",
    "    def add_ctxt(self,row):\n",
    "        try:\n",
    "            row['Context_Answer']=' '.join([self.contents_doc[0][1][int(row.Doc_index)-1],row.Answer,self.contents_doc[0][1][int(row.Doc_index)+1]])\n",
    "        except:\n",
    "            print('No context for index:',row.Doc_index)\n",
    "            row['Context Answer']= ' '\n",
    "        return row\n",
    "    \n",
    "    def update_count_dic(self,sentences,counter,is_rawtext,remove_index):\n",
    "        \n",
    "        for i,c in enumerate(sentences):\n",
    "            counter=counter[is_rawtext].copy()\n",
    "            counter[c]=counter.get(c,0)+1\n",
    "            counter[is_rawtext]=counter\n",
    "            if counter[is_rawtext][c]>1:\n",
    "                remove_index[is_rawtext].append(i)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose your args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_args():\n",
    "\n",
    "    if not FLAGS.demo:              \n",
    "        path_q=FLAGS.query \n",
    "        file= open(path_q,\"r+\")  \n",
    "        text=file.read().replace(\"  \",\"\")\n",
    "        queries=text.split(\"\\n\")\n",
    "        queries=[q.split(\"\\t\")[0] for q in queries if len(q)>1]\n",
    "        topics=[q.split(\"\\t\")[1].split(\",\") for q in queries if len(q)>1]\n",
    "        file.close()\n",
    "        \n",
    "    else:\n",
    "        queries=[FLAGS.demo_query]\n",
    "        if FLAGS.model==5:\n",
    "            topics=FLAGS.demo_topics\n",
    "            topics=[topics.split(\",\")]\n",
    "        \n",
    "    pdf_dirs=[FLAGS.pdf_directory+FLAGS.domain+'/'+FLAGS.retrieved_company]\n",
    "    grams=(FLAGS.min_gram,FLAGS.max_gram)\n",
    "    \n",
    "    args_Infersent={'pdf_directories':pdf_dirs,'w2v_path': FLAGS.w2v_path, 'model_path': FLAGS.model_path ,'top_n':FLAGS.top_n,'ngram_range':grams,'lemmatize':FLAGS.lemmatize,'transform_text':FLAGS.transform_text,'l_questions':queries,'sentences_chunk':FLAGS.sentences_chunk,'vocab_builder':FLAGS.vocab_builder}\n",
    "    args_Bert={'pdf_directories':pdf_dirs,'w2v_path': FLAGS.w2v_path, 'model_path': FLAGS.model_path ,'top_n':FLAGS.top_n,'ngram_range':grams,'lemmatize':FLAGS.lemmatize,'transform_text':FLAGS.transform_text,'l_questions':queries,'sentences_chunk':FLAGS.sentences_chunk,'vocab_builder':FLAGS.vocab_builder}\n",
    "    args_Tf_Idf={'pdf_directories':pdf_dirs,'w2v_path': FLAGS.w2v_path, 'model_path': FLAGS.model_path ,'top_n':FLAGS.top_n,'ngram_range':grams,'lemmatize':FLAGS.lemmatize,'transform_text':FLAGS.transform_text,'l_questions':queries,'sentences_chunk':FLAGS.sentences_chunk,'vocab_builder':FLAGS.vocab_builder}\n",
    "    args_TfBERT={'pdf_directories':pdf_dirs,'w2v_path': FLAGS.w2v_path, 'model_path': FLAGS.model_path ,'top_n':FLAGS.top_n,'ngram_range':grams,'lemmatize':FLAGS.lemmatize,'transform_text':FLAGS.transform_text,'l_questions':queries,'sentences_chunk':FLAGS.sentences_chunk,'vocab_builder':FLAGS.vocab_builder}\n",
    "    args_TfBERT_enhanced={'pdf_directories':pdf_dirs,'w2v_path': FLAGS.w2v_path, 'model_path': FLAGS.model_path ,'top_n':FLAGS.top_n,'ngram_range':grams,'lemmatize':FLAGS.lemmatize,'transform_text':FLAGS.transform_text,'l_questions':queries,'sentences_chunk':FLAGS.sentences_chunk,'vocab_builder':FLAGS.vocab_builder,'topics':topics}\n",
    "    args_All_transforms={'pdf_directories':pdf_dirs,'w2v_path': FLAGS.w2v_path, 'model_path': FLAGS.model_path ,'top_n':FLAGS.top_n,'ngram_range':grams,'lemmatize':FLAGS.lemmatize,'transform_text':FLAGS.transform_text,'l_questions':queries,'sentences_chunk':FLAGS.sentences_chunk,'vocab_builder':FLAGS.vocab_builder,'topics':topics}\n",
    "\n",
    "    if FLAGS.model==1:\n",
    "        return args_Infersent\n",
    "    elif FLAGS.model==2:\n",
    "        return args_Bert\n",
    "    elif FLAGS.model==3:\n",
    "        return args_Tf_Idf\n",
    "    elif FLAGS.model==4:\n",
    "        return args_TfBERT\n",
    "    elif FLAGS.model==5:\n",
    "        return args_TfBERT_enhanced\n",
    "    else:\n",
    "        print('Select a correct model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*********** READER *************\n",
      "Reading pdfs doc on location: ./utils/pdf_files/Tourism/Disney/pdfs/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-58c242a59e83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtabulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'keys'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtablefmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'psql'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-58c242a59e83>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollect_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mQAmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQApipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mQAmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQAmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l_questions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-7ff0c9fd1af5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}*********** READER *************'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reading pdfs doc on location: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf_directory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieved_company\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/pdfs/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpdfconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs_converter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m#BUILD CONTENT AND DOCUMENT INDEX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/harshQA/harshQA_pdf_reader/reader.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0;34m\"\"\" 1- Handle linebreaks to optimize TextBlob.sentences results\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                     \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreat_new_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                     \u001b[0;34m\"\"\" 2- Divide text by sentences using TextBlob\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/harshQA/harshQA_pdf_reader/reader.py\u001b[0m in \u001b[0;36mtreat_new_line\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    307\u001b[0m                             \u001b[0mprec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                             \u001b[0mmerge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                             \u001b[0mdic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m                             \u001b[0mproper_noun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_w\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'NNP'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproper_noun\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m    161\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    117\u001b[0m         )\n\u001b[1;32m    118\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Maps to the specified tagset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'eng'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_conf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, features, return_conf)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(_):\n",
    "    \n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    dic_suffix={1:'FASTEXT',2:'BERT',3:'TFIDF',4:'BERTCLUST',5:'BERTCLUST_TUNED'}\n",
    "    \n",
    "    check_args()\n",
    "    args={ key:value for key,value in collect_args() if key!=\"l_questions\"}\n",
    "    QAmodel=QApipeline(**args) \n",
    "    QAmodel.fit()\n",
    "    results=QAmodel.predict(args['l_questions'])\n",
    "    \n",
    "    if not FLAGS.demo:\n",
    "        results.to_csv(FLAGS.pdf_directory+FLAGS.domain+'/'+FLAGS.retrieved_company+'/results/' +FLAGS.retrieved_company+dic_suffix[FLAGS.model]+'.csv')\n",
    "    else:\n",
    "        print('*******************************  RESULTS    ***************************************')\n",
    "        show=results[['Score','Answer']]\n",
    "        print(tabulate(show, headers='keys', tablefmt='psql'))\n",
    "        \n",
    "tf.compat.v1.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
